{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CSC413_project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Nxqu1UeZKXjC"
      ],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/arnaidu/RNN-Attention-COVID19/blob/main/CSC413_project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWY7Av9lYSxu"
      },
      "source": [
        "# CSC413 Project Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cEzHzWcf5ScC"
      },
      "source": [
        "# Utils"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sFXQl4ewBWW0"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "import abc \n",
        "from itertools import combinations, permutations\n",
        "from matplotlib import pyplot as plt\n",
        "import matplotlib.ticker as ticker\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "class AttrDict(dict):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(AttrDict, self).__init__(*args, **kwargs)\n",
        "        self.__dict__ = self\n",
        "\n",
        "for i in range(1, 5): ## make directories for experiment\n",
        "  directory = 'experiment{}'.format(i)\n",
        "  try:\n",
        "    os.stat(directory)\n",
        "  except:\n",
        "      os.mkdir(directory)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWLdtWVqicM7"
      },
      "source": [
        "# Import Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkU228kiav_y",
        "outputId": "1277134e-762c-440d-df39-620e083e29bf"
      },
      "source": [
        "## Import csv file\n",
        "url = \"https://data.ontario.ca/dataset/f4f86e54-872d-43f8-8a86-3892fd3cb5e6/resource/ed270bb8-340b-41f9-a7c6-e8ef587e6d11/download/covidtesting.csv\"\n",
        "df = pd.read_csv(url)\n",
        "## Delete the last 3 ('Total_Lineage...') columns\n",
        "df = df.drop(columns=['Total_Lineage_B.1.1.7', 'Total_Lineage_B.1.351', 'Total_Lineage_P.1'])\n",
        "## Also delete the 'legacy variables' (ie, Confirmed Negative, Presumptive Positive, Presumptive Negative) -- these were only reported at the very beginning\n",
        "df = df.drop(columns=['Confirmed Negative', 'Presumptive Positive', 'Presumptive Negative'])\n",
        "## Also delete the \"new\" stats (only started reporting from March 30, 2021)\n",
        "df = df.drop(columns=['Number of patients in ICU, testing positive for COVID-19', 'Number of patients in ICU, testing negative for COVID-19', 'Num. of patients in ICU on a ventilator testing positive', 'Num. of patients in ICU on a ventilator testing negative'])\n",
        "\n",
        "print(df.columns)\n",
        "['Total Cases', 'Deaths'] ## cumulative\n",
        "['Positive', 'New Deaths'] ## not cumulative\n",
        "\n",
        "BASIC_FEATURES = ['Positive', 'Total Cases', 'Resolved', 'Deaths']\n",
        "TEST_FEATURES = ['AFT','TTC', 'PPT', 'UI']\n",
        "HOSPITAL_FEATURES = ['Hospital', 'ICU', 'Ventilator']\n",
        "LTC_FEATURES = ['TPLTCR', 'TPLTCHCW', 'TLCTRD', 'TLTCHCWD']\n",
        "NEW_LTC_FEATURES = ['PLTCR', 'PLTCHCW', 'LCTRD', 'LTCHCWD']\n",
        "\n",
        "## Rename comlumns for simplicity\n",
        "'''\n",
        "AFT = Total patients approved for testing as of Reporting Date;\n",
        "TTC = Total tests completed in the last day;\n",
        "PPT = Percent positive tests in last day; UI = Under Investigation;\n",
        "Hospital = Number of patients hospitalized with COVID-19;\n",
        "ICU = Number of patients in ICU due to COVID-19;\n",
        "Ventilator = Number of patients in ICU on a ventilator due to COVID-19;\n",
        "TPLTCR = Total Positive LTC Resident Cases;\n",
        "TPLTCHCW = Total Positive LTC HCW Cases;\n",
        "TLCTRD = Total LTC Resident Deaths;\n",
        "TLTCHCWD = Total LTC HCW Deaths\n",
        "'''\n",
        "df = df.rename(columns={'Confirmed Positive': 'Positive',\n",
        "'Total patients approved for testing as of Reporting Date': 'AFT',\n",
        "'Total tests completed in the last day': 'TTC',\n",
        "'Percent positive tests in last day': 'PPT', 'Under Investigation': 'UI',\n",
        "'Number of patients hospitalized with COVID-19': 'Hospital',\n",
        "'Number of patients in ICU due to COVID-19': 'ICU',\n",
        "'Number of patients in ICU on a ventilator due to COVID-19': 'Ventilator',\n",
        "'Total Positive LTC Resident Cases': 'TPLTCR',\n",
        "'Total Positive LTC HCW Cases': 'TPLTCHCW',\n",
        "'Total LTC Resident Deaths': 'TLCTRD', 'Total LTC HCW Deaths': 'TLTCHCWD'})\n",
        "\n",
        "## Delete data before May 19, 2020 (day that all stats have at least one entry)\n",
        "df = df.drop(df[df['Reported Date'] < '2020-05-19'].index)\n",
        "df = df.reset_index(drop=True)\n",
        "df_date = df['Reported Date']\n",
        "df = df.drop(columns=['Reported Date'])\n",
        "\n",
        "# Interpolate middle NaNs with average of closest known values\n",
        "df = df.interpolate(method='linear', axis=0, limit_direction='both')\n",
        "\n",
        "## Change cumulative counts to differences for LTC features\n",
        "df[NEW_LTC_FEATURES] = df[LTC_FEATURES].sub(df[LTC_FEATURES].shift())\n",
        "\n",
        "## Change cumulative death counts to differences\n",
        "df['New Deaths'] = df['Deaths'].sub(df['Deaths'].shift())\n",
        "\n",
        "## Replace negative values with 0 -- improper cumulative counts\n",
        "df[df < 0] = 0\n",
        "df['Reported Date'] = df_date\n",
        "df = df[1:]\n",
        "print(df.columns)\n",
        "\n",
        "## Reorder columns\n",
        "# df = df[['Positive', 'Total Cases', 'Resolved', 'Deaths', 'New Deaths'] + TEST_FEATURES + HOSPITAL_FEATURES + LTC_FEATURES + NEW_LTC_FEATURES + ['Reported Date']]\n",
        "\n",
        "df = df[['Positive', 'Total Cases', 'Resolved', 'Deaths'] + TEST_FEATURES + HOSPITAL_FEATURES + LTC_FEATURES + ['Reported Date']]\n",
        "\n",
        "\n",
        "ALL_DATES = df['Reported Date'].values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Index(['Reported Date', 'Confirmed Positive', 'Resolved', 'Deaths',\n",
            "       'Total Cases',\n",
            "       'Total patients approved for testing as of Reporting Date',\n",
            "       'Total tests completed in the last day',\n",
            "       'Percent positive tests in last day', 'Under Investigation',\n",
            "       'Number of patients hospitalized with COVID-19',\n",
            "       'Number of patients in ICU due to COVID-19',\n",
            "       'Number of patients in ICU on a ventilator due to COVID-19',\n",
            "       'Total Positive LTC Resident Cases', 'Total Positive LTC HCW Cases',\n",
            "       'Total LTC Resident Deaths', 'Total LTC HCW Deaths'],\n",
            "      dtype='object')\n",
            "Index(['Positive', 'Resolved', 'Deaths', 'Total Cases', 'AFT', 'TTC', 'PPT',\n",
            "       'UI', 'Hospital', 'ICU', 'Ventilator', 'TPLTCR', 'TPLTCHCW', 'TLCTRD',\n",
            "       'TLTCHCWD', 'PLTCR', 'PLTCHCW', 'LCTRD', 'LTCHCWD', 'New Deaths',\n",
            "       'Reported Date'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2D-nJR4K8ce"
      },
      "source": [
        "# Preprocess Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytSKpJ940Ocb"
      },
      "source": [
        "## Cluster Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIYEIlmZuU7a"
      },
      "source": [
        "def cluster_data(df, k, t, in_features=None, out_features=['Positive', 'Resolved', 'New Deaths'],\n",
        "                 separate=True, moving_average=False, m=3):\n",
        "  \"\"\"Cluster data for t days ahead prediction with condition window k\n",
        "  ie, target is t days ahead of the last day in the condition window\n",
        "  \n",
        "  df: Dataframe of cleaned data\n",
        "  k: Condition window size\n",
        "  t: Number of days ahead for prediction\n",
        "  in_features: List of input features to be in X\n",
        "  out_features: List of output features to be in y\n",
        "  moving_average: Whether or not to model the moving average (form of data smoothing)\n",
        "      - Then, the condition window will be k consecutive moving averages and we predict \n",
        "      the t day's ahead moving average\n",
        "  m: If moving_average is True, the number of days involved in the moving average\n",
        "\n",
        "  return: \n",
        "  - X (# time steps - t, t, len(in_features)) -- torch.Tensor (note: on cpu; must put on gpu in a later step)\n",
        "     - if in_features is None, in_features = all features\n",
        "  - y (# time steps - t, len(out_features)) -- torch.Tensor\n",
        "  \"\"\"\n",
        "  df_date = df['Reported Date']\n",
        "  df = df.drop(columns='Reported Date')\n",
        "\n",
        "  ## Note slicing in pandas (in this case), is inclusive of the end index\n",
        "  if separate:\n",
        "      if moving_average:\n",
        "          if in_features is None:\n",
        "              in_features = list(set(df.columns) - set(out_features))\n",
        "          clustered_in = torch.as_tensor([df.loc[s:s+m-1, in_features].values for s in range(1, len(df) - m + 2)])\n",
        "          clustered_in = torch.mean(clustered_in, dim=1).squeeze()\n",
        "          clustered_out = torch.as_tensor([df.loc[s:s+m-1, out_features].values for s in range(1, len(df) - m + 2)])\n",
        "          clustered_out = torch.mean(clustered_out, dim=1).squeeze()\n",
        "          X = torch.stack([clustered_in[s:s+k] for s in range(len(clustered_in) - t - k + 1)], dim=0)\n",
        "          X_tilde = torch.stack([clustered_out[s:s+k] for s in range(len(clustered_out) - t - k + 1)], dim=0)\n",
        "          y = clustered_out[k + t - 1:]\n",
        "          y_cumul = torch.stack([clustered_out[d:d+t] for d in range(k, len(clustered_out) - t + 1)], dim=0).sum(dim=1)\n",
        "          dates = df_date.loc[(m // 2) + k + t:].values[:-m//2 + 1]\n",
        "      else:\n",
        "          if in_features is None:\n",
        "              in_features = list(set(df.columns) - set(out_features))\n",
        "          X = torch.as_tensor([df.loc[s:s+k-1, in_features].values for s in range(1, len(df) - t - k + 2)])\n",
        "          X_tilde = torch.as_tensor([df.loc[s:s+k-1, out_features].values for s in range(1, len(df) - t - k + 2)])\n",
        "          y = torch.as_tensor(df[out_features].loc[k + t:].values)\n",
        "          y_cumul = torch.stack([torch.as_tensor(df[out_features].loc[d:d+t-1].values) for d in range(k+1, len(df) - t + 2)], dim=0).sum(dim=1)\n",
        "          dates = df_date.loc[k + t:].values\n",
        "  else:\n",
        "      if moving_average:\n",
        "          if in_features is not None:\n",
        "              clustered_in = torch.as_tensor([df.loc[s:s+m-1, in_features].values for s in range(1, len(df) - m + 2)])\n",
        "          else:\n",
        "              clustered_in = torch.as_tensor([df.loc[s:s+m-1].values for s in range(1, len(df) - m + 2)])\n",
        "          clustered_in = torch.mean(clustered_in, dim=1).squeeze()\n",
        "          clustered_out = torch.as_tensor([df.loc[s:s+m-1, out_features].values for s in range(1, len(df) - m + 2)])\n",
        "          clustered_out = torch.mean(clustered_out, dim=1).squeeze()\n",
        "          X = torch.stack([clustered_in[s:s+k] for s in range(len(clustered_in) - t - k + 1)], dim=0)\n",
        "          y = clustered_out[k + t - 1:]\n",
        "          y_cumul = torch.stack([clustered_out[d:d+t] for d in range(k, len(clustered_out) - t + 1)], dim=0).sum(dim=1)\n",
        "          dates = df_date.loc[(m // 2) + k + t:].values[:-m//2 + 1]\n",
        "      else:\n",
        "          if in_features is None:\n",
        "              in_features = df.columns()\n",
        "          X = torch.as_tensor([df.loc[s:s+k-1, in_features].values for s in range(1, len(df) - t - k + 2)])\n",
        "          y = torch.as_tensor(df[out_features].loc[k + t:].values)\n",
        "          y_cumul = torch.stack([torch.as_tensor(df[out_features].loc[d:d+t-1].values) for d in range(k+1, len(df) - t + 2)], dim=0).sum(dim=1)\n",
        "          dates = df_date.loc[k + t:].values\n",
        "      X_tilde = None\n",
        "\n",
        "  if len(X.shape) == 2: ## encoder input only 1 feature\n",
        "      X = X.unsqueeze(-1)\n",
        "  if X_tilde is not None and len(X_tilde.shape) == 2: ## decoder input only 1 feature\n",
        "      X_tilde = X_tilde.unsqueeze(-1)\n",
        "  if len(y.shape) == 1: ## output only 1 feature\n",
        "      y = y.unsqueeze(-1)\n",
        "      y_cumul = y_cumul.unsqueeze(-1)\n",
        "\n",
        "  return X, X_tilde, y, y_cumul, dates, in_features\n",
        "\n",
        "# You can uncomment this to see how the data looks like\n",
        "# X, X_tilde, y, y_cumul, dates = cluster_data(df, in_features=['New Deaths'], out_features=['New Deaths'], k=5, t=7, moving_average=True, m=3)\n",
        "# print(X[:10])\n",
        "# print(y_cumul)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9OJWKBsJK_pY"
      },
      "source": [
        "## Dataset and DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JwICkr_eLCGm"
      },
      "source": [
        "class CovidDataset(Dataset):\n",
        "  def __init__(self, X, X_tilde, y):\n",
        "    self.X = X\n",
        "    self.X_tilde = X_tilde\n",
        "    self.y = y\n",
        "  \n",
        "  def __len__(self):\n",
        "    return self.X.shape[0]\n",
        "  \n",
        "  def __getitem__(self, i):\n",
        "    if self.X_tilde is None:\n",
        "      return self.X[i], self.y[i]\n",
        "    else:\n",
        "      return self.X[i], self.X_tilde[i], self.y[i]\n",
        "\n",
        "class CovidDataLoader(DataLoader):\n",
        "  def __init__(self, X, X_tilde, y, batch_size):\n",
        "    dataset = CovidDataset(X, X_tilde, y)\n",
        "    super().__init__(dataset, batch_size=batch_size, shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnC0ZovZjef8"
      },
      "source": [
        "## Train, Validation, Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsnpSk_yjgjA"
      },
      "source": [
        "def train_val_test_split(X, X_tilde, y, dates):\n",
        "    \"\"\"Splits dataset into training, validation and testing\"\"\"\n",
        "\n",
        "    X, y = X.float(), y.float()\n",
        "    jan_1_idx = np.where(dates == '2021-01-01')[0][0]\n",
        "    march_1_idx = np.where(dates == '2021-03-01')[0][0]\n",
        "    april_17_idx = np.where(dates == '2021-04-17')[0][0]\n",
        "\n",
        "    X_train = X[:jan_1_idx]\n",
        "    y_train = y[:jan_1_idx]\n",
        "    X_val = X[jan_1_idx:march_1_idx]\n",
        "    y_val = y[jan_1_idx:march_1_idx]\n",
        "    X_test = X[march_1_idx:april_17_idx]\n",
        "    y_test = y[march_1_idx:april_17_idx]\n",
        "\n",
        "    if X_tilde is not None:\n",
        "        X_tilde = X_tilde.float()\n",
        "        X_tilde_train = X_tilde[:jan_1_idx]\n",
        "        X_tilde_val = X_tilde[jan_1_idx:march_1_idx]\n",
        "        X_tilde_test = X_tilde[march_1_idx:april_17_idx]\n",
        "    else:\n",
        "        X_tilde_train, X_tilde_val, X_tilde_test = None, None, None\n",
        "\n",
        "    return X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, X_test, X_tilde_test, y_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adtti4Ami3YY"
      },
      "source": [
        "# Different types of RNN architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBv3f9NwjEVS"
      },
      "source": [
        "## MyRNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LjZ6J9iphy7G"
      },
      "source": [
        "class MyRNN(nn.Module):\n",
        "    def __init__(self, rnn_class, input_size, hidden_size, output_size, num_layers, seq_length, dropout, bidirectional=False, cell_type=\"VanillaRNN\", attn_type=None, temporal_attn=True):\n",
        "    \n",
        "        super().__init__()\n",
        "        self.rnn = rnn_class(input_size, hidden_size, num_layers, seq_length, dropout, bidirectional, cell_type, attn_type)\n",
        "        self.attn_type = attn_type\n",
        "        self.forward_attn_weights = None\n",
        "        self.backward_attn_weights = None\n",
        "        self.temporal_attn = temporal_attn\n",
        "        self.temporal_attn_weights = None\n",
        "        self.bidirectional = bidirectional\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        if bidirectional:\n",
        "            self.temporal_attn_layer = nn.Sequential(nn.Linear(hidden_size * 2, hidden_size), nn.Tanh(), nn.Linear(hidden_size, 1), nn.Softmax(dim=1))\n",
        "            self.fc = nn.Linear(hidden_size*2, output_size) \n",
        "        else:\n",
        "            self.temporal_attn_layer = nn.Sequential(nn.Linear(hidden_size, hidden_size // 2), nn.Tanh(), nn.Linear(hidden_size // 2, 1), nn.Softmax(dim=1))\n",
        "            self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "\n",
        "    def get_attention_weights(self):\n",
        "        return (self.forward_attn_weights, self.backward_attn_weights), self.temporal_attn_weights\n",
        "\n",
        "    def forward(self, X, X_tilde):\n",
        "        if self.attn_type is None:\n",
        "            output = self.rnn(X)\n",
        "        else:\n",
        "            output, self.forward_attn_weights, self.backward_atnn_weights = self.rnn(X)\n",
        "        \n",
        "        if self.temporal_attn:\n",
        "            self.temporal_attn_weights = self.temporal_attn_layer(output) ## (B, S, 1)\n",
        "            output = (self.temporal_attn_weights.transpose(2,1) @ output).squeeze() ## (B, 1, S) * (B, S, H) = (B, 1, H)\n",
        "        else:\n",
        "            if self.bidirectional:\n",
        "                h_forward = output[:, -1, :self.hidden_size]\n",
        "                h_backward = output[:, 0, self.hidden_size:]\n",
        "                output = torch.cat([h_forward, h_backward], dim=1)\n",
        "            else:\n",
        "                output = output[:, -1, :, :]\n",
        "\n",
        "        output = self.fc(output)\n",
        "        ## Unnormalize output (we assume that the output features are the first ones in the input)\n",
        "        output = X_tilde.mean(dim=1) + output * X_tilde.std(dim=1)\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lZgvZ9lZh_Oz"
      },
      "source": [
        "## RNN Without Input Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAQ69niXX4gU"
      },
      "source": [
        "class RNNWithoutAttention(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, seq_len, dropout, bidirectional=False, cell_type=\"VanillaRNN\", attn_type=None):\n",
        "    super().__init__()\n",
        "    self.hidden_size = hidden_size\n",
        "    self.num_layers = num_layers\n",
        "    self.bidirectional = bidirectional\n",
        "\n",
        "    if cell_type == \"VanillaRNN\":\n",
        "      self.rnn = nn.RNN(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "    elif cell_type == \"LSTM\":\n",
        "      self.rnn = nn.LSTM(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "    else:\n",
        "      self.rnn = nn.GRU(input_size, hidden_size, num_layers, dropout=dropout, batch_first=True, bidirectional=bidirectional)\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      data = the initial data of size (batch_size, seq_length, num_input_features)\n",
        "    Output:\n",
        "      output = all hidden states\n",
        "    \"\"\"\n",
        "    ## Note: initial hidden/cell states default to 0\n",
        "\n",
        "    ## Normalize input data\n",
        "    mean = X.mean(dim=1).unsqueeze(1)\n",
        "    std = X.std(dim=1).unsqueeze(1)\n",
        "    data = (X - mean) / (std + 1e-7)\n",
        "\n",
        "    output = self.rnn(data)[0]\n",
        "    \n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9pM5ixc8atec"
      },
      "source": [
        "## RNN with Input Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4JGZXgy6av_J"
      },
      "source": [
        "class RNNWithAttention(nn.Module):\n",
        "  def __init__(self, input_size, hidden_size, num_layers, seq_len, dropout, bidirectional=False, cell_type=\"VanillaRNN\", attn_type=\"Additive\"):\n",
        "    \n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.seq_len = seq_len\n",
        "    self.num_hidden_layers = num_layers\n",
        "    self.hidden_state_size = hidden_size\n",
        "    self.dropout = dropout\n",
        "    self.cell_type = cell_type\n",
        "    self.attn_type = attn_type\n",
        "    self.bidirectional = bidirectional\n",
        "    self.init_submodules()\n",
        "\n",
        "\n",
        "  def init_submodules(self):\n",
        "    \"\"\"Initialize the submodules for encoder with attention\"\"\"\n",
        "    \n",
        "    if self.cell_type == \"VanillaRNN\":\n",
        "      net = [nn.RNNCell(input_size=self.input_size, hidden_size=self.hidden_state_size)]\n",
        "      net.extend([nn.RNNCell(input_size=self.hidden_state_size, hidden_size=self.hidden_state_size)\n",
        "                                  for _ in range(self.num_hidden_layers - 1)])\n",
        "    elif self.cell_type == \"LSTM\":\n",
        "      net = [nn.LSTMCell(input_size=self.input_size, hidden_size=self.hidden_state_size)]\n",
        "      net.extend([nn.LSTMCell(input_size=self.hidden_state_size, hidden_size=self.hidden_state_size)\n",
        "                                  for _ in range(self.num_hidden_layers - 1)])\n",
        "    \n",
        "    else:\n",
        "      net = [nn.GRUCell(input_size=self.input_size, hidden_size=self.hidden_state_size)]\n",
        "      net.extend([nn.GRUCell(input_size=self.hidden_state_size, hidden_size=self.hidden_state_size)\n",
        "                                  for _ in range(self.num_hidden_layers - 1)])\n",
        "    \n",
        "    self.rnn_forward = nn.Sequential(*net)\n",
        "\n",
        "    self.project_to_input_dim_forward = nn.Linear(self.hidden_state_size, self.input_size)\n",
        "    self.project_to_input_dim_backward = nn.Linear(self.hidden_state_size, self.input_size)\n",
        "\n",
        "    if self.attn_type == \"Additive\":\n",
        "      attn = [AdditiveAttention(hidden_size=self.seq_len, encoder=True) for _ in range(self.num_hidden_layers)]\n",
        "    elif self.attn_type == \"Scaled-Dot\":\n",
        "      attn = [ScaledDotAttention(hidden_size=self.seq_len, hidden_size2=self.input_size, encoder=True)]\n",
        "      attn.extend([ScaledDotAttention(hidden_size=self.seq_len, hidden_size2 = self.hidden_state_size, encoder=True) for _ in range(self.num_hidden_layers - 1)])\n",
        "    elif self.attn_type == \"Linear\":\n",
        "      attn = [LinearAttention(hidden_size=self.seq_len, encoder=True) for _ in range(self.num_hidden_layers)]\n",
        "    else:\n",
        "      attn = [DaRnnAttention(hidden_size=self.seq_len) for _ in range(self.num_hidden_layers)]\n",
        "\n",
        "    self.attn_forward = nn.Sequential(*attn)\n",
        "\n",
        "    if self.bidirectional:\n",
        "      self.rnn_backward = nn.Sequential(*net.copy())\n",
        "      self.attn_backward = nn.Sequential(*attn.copy()) \n",
        "    \n",
        "    self.forward_attn_weights = None\n",
        "    self.backward_attn_weights = None\n",
        "\n",
        "  def forward(self, X):\n",
        "    \"\"\"Forward pass of data in RNN with input attention\"\"\"\n",
        "    # Normalize input data\n",
        "    mean = X.mean(dim=1).unsqueeze(1)\n",
        "    std = X.std(dim=1).unsqueeze(1)\n",
        "    data = (X - mean) / (std + 1e-7)\n",
        "\n",
        "    forward_output, self.forward_attn_weights = self.forward_pass(data)\n",
        "    if self.bidirectional: ## Get last hidden state\n",
        "      backward_output, self.backward_attn_weights = self.backward_pass(data)\n",
        "      output = torch.cat([forward_output, backward_output], dim=2)\n",
        "    else:\n",
        "      output = forward_output\n",
        "    \n",
        "    return output, self.forward_attn_weights, self.backward_attn_weights\n",
        "\n",
        "  def forward_pass(self, data, direction='Forward'):\n",
        "    \"\"\"Forward pass of rnn with attention in the forward direction\"\"\"\n",
        "    \n",
        "    attentions_all_layers = []\n",
        "\n",
        "    if direction == 'Forward':\n",
        "        attn_net = self.attn_forward\n",
        "        rnn = self.rnn_forward\n",
        "        project_to_input_dim = self.project_to_input_dim_forward\n",
        "    else:\n",
        "        attn_net = self.attn_backward\n",
        "        rnn = self.rnn_backward\n",
        "        project_to_input_dim = self.project_to_input_dim_backward\n",
        "\n",
        "    ## Iterate over all layers\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      attentions = []\n",
        "      \n",
        "      ## Get first hidden state\n",
        "      h_prev = self.get_first_hidden_state(data) ## (batch_size, input_size) or (batch_size, hidden_size) \n",
        "\n",
        "      ## Loop over the sequence\n",
        "      hiddens = []\n",
        "      for j in range(self.seq_len):\n",
        "        current_input = data[:, j, :] ## Get input at current time step (batch_size, input_size) or (batch_size, hidden_size)\n",
        "        data = data.transpose(1, 2) ## (batch_size, input_size, seq_len) or (batch_size, hidden_size, seq_len)\n",
        "\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            if h_prev[0].shape[1] != data.shape[1]:\n",
        "                h_prev_pr = project_to_input_dim(h_prev[0]) ## (batch_size, input_size)\n",
        "            else:\n",
        "                h_prev_pr = h_prev[0]\n",
        "        else:\n",
        "            if h_prev.shape[1] != data.shape[1]:\n",
        "                h_prev_pr = project_to_input_dim(h_prev)\n",
        "            else:\n",
        "                h_prev_pr = h_prev\n",
        "\n",
        "        h_repeated = h_prev_pr.unsqueeze(-1).expand_as(data)  ## (batch_size, input_size, seq_len)\n",
        "\n",
        "        ## Pass through attention layer\n",
        "        _, attention_weights = attn_net[i](h_repeated, data, data) ## (batch_size, input_size, 1) or (batch_size, hidden_size, 1)\n",
        "        ## Pass through rnn\n",
        "        weighted_input = attention_weights.squeeze(2) * current_input\n",
        "        h_prev = rnn[i](weighted_input, h_prev) ## (batch_size, hidden_size)\n",
        "        \n",
        "        ## Keep important stuff in arrays\n",
        "        attentions.append(attention_weights) ## (batch_size, hidden_size) \n",
        "        if self.cell_type == \"LSTM\":\n",
        "          hiddens.append(h_prev[0]) ## (batch_size, hidden_size) \n",
        "        else:\n",
        "          hiddens.append(h_prev)\n",
        "\n",
        "        ## Revert back to previous orientation\n",
        "        data = data.transpose(1, 2) ## (batch_size, seq_len, input_size) or (batch_size, seq_len, hidden_size)\n",
        "      \n",
        "      ## outputs of previous layer are inputs to the next layer\n",
        "      data = torch.stack(hiddens, dim=1) ## (batch_size, seq_len, hidden_size)\n",
        "      attentions = torch.cat(attentions, dim=2) ## (batch_size, hidden_size, seq_len)\n",
        "      attentions_all_layers.append(attentions)\n",
        "    \n",
        "    final_hiddens = data\n",
        "    ## attentions_all_layers is a list of num_layers tensors of size (batch_size, input or hidden_size, seq_len)\n",
        "    return final_hiddens, attentions_all_layers\n",
        "  \n",
        "  def backward_pass(self, data):\n",
        "      \"\"\"RNN in the reverse direction\"\"\"\n",
        "      ## Reverse the input data along the sequence dimension\n",
        "      data = data.flip(dims=(1,))\n",
        "\n",
        "      return self.forward_pass(data, direction='Backward')\n",
        "\n",
        "  def get_first_hidden_state(self, encoder_input):\n",
        "    ## Just return tensor of zeros or tuple if LSTM\n",
        "    ## Return shape: (batch_size, hidden_size)\n",
        "    htilde_tm1 = torch.zeros((encoder_input.shape[0], self.hidden_state_size), device=encoder_input.device)\n",
        "    if self.cell_type == \"LSTM\":\n",
        "      return htilde_tm1, torch.zeros_like(htilde_tm1)\n",
        "    else:\n",
        "      return htilde_tm1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIdHHQc0zpuX"
      },
      "source": [
        "# Encoder-Decoder Architectures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rorHaMfKFXl"
      },
      "source": [
        "## EncoderDecoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vmwhq0kmKHUl"
      },
      "source": [
        "class EncoderDecoder(nn.Module):\n",
        "  def __init__(self, encoder_class, decoder_class, input_size, seq_len, encoder_num_hidden_layers, \n",
        "               hidden_state_size, output_size, dropout, cell_type,\n",
        "               enc_attn_type, decoder_num_hidden_layers, dec_attn_type, bidirectional, temporal_attn=True):\n",
        "    super().__init__()\n",
        "    self.encoder = encoder_class(input_size, seq_len, encoder_num_hidden_layers, \n",
        "               hidden_state_size, dropout=dropout, cell_type=cell_type, attn_type=enc_attn_type)\n",
        "    \n",
        "    ## Bidirectional\n",
        "    if bidirectional:\n",
        "        self.decoder = decoder_class(output_size, seq_len, decoder_num_hidden_layers, hidden_state_size * 2, cell_type=cell_type, attn_type=dec_attn_type)\n",
        "        self.temporal_attn_layer = nn.Sequential(nn.Linear(hidden_state_size * 2, hidden_state_size), nn.Tanh(), nn.Linear(hidden_state_size, 1), nn.Softmax(dim=1))\n",
        "        self.fc = nn.Linear(hidden_state_size * 2, output_size)\n",
        "    else:\n",
        "        self.decoder = decoder_class(output_size, seq_len, decoder_num_hidden_layers, hidden_state_size, cell_type=cell_type, attn_type=dec_attn_type)\n",
        "        self.temporal_attn_layer = nn.Sequential(nn.Linear(hidden_state_size, hidden_state_size // 2), nn.Tanh(), nn.Linear(hidden_state_size // 2, 1), nn.Softmax(dim=1))\n",
        "        self.fc = nn.Linear(hidden_state_size, output_size)\n",
        "\n",
        "    self.enc_attn_type = enc_attn_type\n",
        "    self.dec_attn_type = dec_attn_type\n",
        "\n",
        "    self.input_attn_weights = None\n",
        "    self.temporal_attn_weights = None\n",
        "\n",
        "    self.temporal_attn = temporal_attn\n",
        "\n",
        "  def get_attention_weights(self):\n",
        "      return self.input_attn_weights, self.temporal_attn_weights\n",
        "\n",
        "  def forward(self, encoder_input, decoder_input):\n",
        "    ## Normalize encoder_input\n",
        "    enc_mean = encoder_input.mean(dim=1).unsqueeze(1)\n",
        "    enc_std = encoder_input.std(dim=1).unsqueeze(1)\n",
        "    encoder_input = (encoder_input - enc_mean) / (enc_std + 1e-7)\n",
        "\n",
        "    ## Put through encoder\n",
        "    encoder_output = self.encoder(encoder_input) ## (batch_size, seq_len, hidden_state_size * 2) if no attention\n",
        "    if self.enc_attn_type is not None: ## (batch_size, seq_len, hidden_state_size) with attention\n",
        "        encoder_output, self.input_attn_weights = encoder_output\n",
        "\n",
        "    ## Normalize decoder_input\n",
        "    dec_mean = decoder_input.mean(dim=1).unsqueeze(1)\n",
        "    dec_std = decoder_input.std(dim=1).unsqueeze(1)\n",
        "    decoder_input = (decoder_input - dec_mean) / (dec_std + 1e-7)\n",
        "    ## Put through decoder\n",
        "    decoder_output = self.decoder(encoder_output, decoder_input) ## (batch_size, hidden_state_size * 2) if no attention\n",
        "    if self.dec_attn_type is not None: ## (batch_size, hidden_state_size) if with attention\n",
        "        decoder_output, final_hidden, _ = decoder_output\n",
        "    else:\n",
        "        final_hidden = decoder_output[:, -1, :]\n",
        "\n",
        "    if self.temporal_attn:\n",
        "        self.temporal_attn_weights = self.temporal_attn_layer(decoder_output) ## (B, S, 1)\n",
        "        decoder_output = (self.temporal_attn_weights.transpose(2,1) @ decoder_output).squeeze() ## (B, 1, S) * (B, S, H) = (B, 1, H)\n",
        "    else:\n",
        "        decoder_output = final_hidden\n",
        "\n",
        "    output = self.fc(decoder_output) ## (batch_size, output_size)\n",
        "\n",
        "    ## \"Unnormalize\" the decoder output \n",
        "    output = dec_mean.squeeze(1) + output * dec_std.squeeze(1)\n",
        "\n",
        "    return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MOC5h1u9jAvD"
      },
      "source": [
        "## Encoder Without Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1d_qwo_iYB9G"
      },
      "source": [
        "class EncoderWithoutAttention(nn.Module):\n",
        "  def __init__(self, input_size, seq_len, num_hidden_layers, hidden_state_size, dropout=0.1, cell_type='LSTM', attn_type=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.seq_len = seq_len\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.hidden_state_size = hidden_state_size\n",
        "    self.dropout = dropout\n",
        "    self.cell_type = cell_type\n",
        "    self.attn_type = attn_type\n",
        "    self.init_submodules()\n",
        "\n",
        "  def init_submodules(self):\n",
        "    if type == \"VanillaRNN\":\n",
        "      self.rnn = nn.RNN(self.input_size, self.hidden_state_size, self.num_hidden_layers, dropout=self.dropout, batch_first=True, bidirectional=True)\n",
        "    elif type == \"LSTM\":\n",
        "      self.rnn = nn.LSTM(self.input_size, self.hidden_state_size, self.num_hidden_layers, dropout=self.dropout, batch_first=True, bidirectional=True)\n",
        "    else:\n",
        "      self.rnn = nn.GRU(self.input_size, self.hidden_state_size, self.num_hidden_layers, dropout=self.dropout, batch_first=True, bidirectional=True)\n",
        "      \n",
        "  def forward(self, encoder_input):\n",
        "    return self.rnn(encoder_input)[0] ## returns all hidden states"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOctERAGGTo2"
      },
      "source": [
        "## Decoder Without Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epYcWO3ZGPx6"
      },
      "source": [
        "class DecoderWithoutAttention(nn.Module):\n",
        "  def __init__(self, output_size, seq_len, num_hidden_layers, hidden_state_size, cell_type='LSTM', attn_type=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "    self.seq_len = seq_len\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.hidden_state_size = hidden_state_size ## Note: this should equal 2 * encoder_hidden_state_size\n",
        "    self.cell_type = cell_type\n",
        "    self.attn_type = attn_type\n",
        "    self.init_submodules()\n",
        "\n",
        "  def init_submodules(self):\n",
        "    if self.cell_type == \"VanillaRNN\":\n",
        "      self.rnn = nn.RNN(input_size=self.output_size, hidden_size=self.hidden_state_size, \n",
        "                        num_layers=self.num_hidden_layers, batch_first=True)\n",
        "\n",
        "    elif self.cell_type == \"LSTM\":\n",
        "      self.rnn = nn.LSTM(input_size=self.output_size, hidden_size=self.hidden_state_size, \n",
        "                         num_layers=self.num_hidden_layers, batch_first=True)\n",
        "\n",
        "    else:\n",
        "      self.rnn = nn.GRU(input_size=self.output_size, hidden_size=self.hidden_state_size, \n",
        "                        num_layers=self.num_hidden_layers, batch_first=True)\n",
        "\n",
        "  def forward(self, encoder_output, decoder_input):\n",
        "    htilde_tm1 = self.get_first_hidden_state(encoder_output)\n",
        "    if self.cell_type == 'LSTM':\n",
        "        htilde_tm1 = (htilde_tm1, torch.zeros_like(htilde_tm1, device=htilde_tm1.device)) ## initialize cell state with zeroes\n",
        "    return self.forward_pass(encoder_output, htilde_tm1, decoder_input)\n",
        "\n",
        "  def forward_pass(self, encoder_output, htilde_tm1, decoder_input):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      encoder_output: encoder output of size (batch_size, self.seq_len, self.hidden_state_size)\n",
        "      htilde_tm1: tensors corresponding to previous hidden state of size (batch_size, self.hidden_state_size)\n",
        "                  For LSTM, this is tuple of 2 tensors\n",
        "      decoder_input: the input to the decoder of size (batch_size, self.seq_len, self.output_size)\n",
        "            \n",
        "    Outputs:\n",
        "      htilde_t: hidden state at t = self.seq_len with size of (batch_size, self.hidden_state_size) or tuple of 2 tensors for LSTM\n",
        "    \"\"\"\n",
        "    \n",
        "    if self.cell_type == 'LSTM':\n",
        "        hidden = (htilde_tm1[0], torch.zeros_like(htilde_tm1[0]))\n",
        "    else:\n",
        "        hidden = htilde_tm1\n",
        "    output, _ = self.rnn(decoder_input, hidden)\n",
        "    \n",
        "    return output\n",
        "    \n",
        "  def get_first_hidden_state(self, encoder_output):\n",
        "    \"\"\"Get the first hidden state to the decoder. \n",
        "    Return: First layer is the concatenation of last encoder hidden state in forward and backward directions.\n",
        "            the rest of the layers are zeros.\n",
        "            shape: (num_layers x batch_size x self.hidden_state_size)\n",
        "    \"\"\"\n",
        "    ## Note: only works with BidirectionalEncoderWithAttention\n",
        "    h_forward = encoder_output[:, -1, :self.hidden_state_size//2] ## (batch_size, encoder_hidden_state_size)\n",
        "    h_backward = encoder_output[:, 0, self.hidden_state_size//2:] ## (batch_size, encoder_hidden_state_size)\n",
        "    htilde_tm1 = torch.cat((h_forward, h_backward), dim=1).unsqueeze(0) ## (1, batch_size, encoder_hidden_state_size*2)\n",
        "    htilde_tm1_rest = torch.zeros((self.num_hidden_layers - 1, encoder_output.shape[0], self.hidden_state_size), device=encoder_output.device) ## (num_layers - 1, batch_size, encoder_hidden_state_size*2)\n",
        "    htilde_tm1 = torch.cat((htilde_tm1, htilde_tm1_rest), dim=0) ## (num_layers, batch_size, encoder_hidden_state_size*2)\n",
        "    return htilde_tm1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c5_8I0yCLqqi"
      },
      "source": [
        "## Encoder with Attention\n",
        "Stacked RNN with input attention\n",
        "\n",
        "Note: the encoder and decoder from PA3 loop over time steps because we coded the LSTM Cell (not LSTM neural net)\n",
        "- Implicitly, torch.nn.LSTM is the \"unrolled\" neural net, whereas torch.nn.LSTMCell is a single cell (similar to what we coded in PA3)\n",
        "- Since, we are applying attention to the input and that depends on the previous hidden (and cell) state(s), we must loop over each time step here\n",
        "   - This goes similarly for the decoder, where we concatenate a context vector to the current input vector before applying the LSTM layer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cTPx0LKj0I_4"
      },
      "source": [
        "### Unidirectional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "suLK61sHL3tx"
      },
      "source": [
        "class EncoderWithAttention(nn.Module):\n",
        "  def __init__(self, input_size, seq_len, num_hidden_layers, hidden_state_size, dropout=0.1, cell_type='LSTM', attn_type=None):\n",
        "    super().__init__()\n",
        "\n",
        "    self.input_size = input_size\n",
        "    self.seq_len = seq_len\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.hidden_state_size = hidden_state_size\n",
        "    self.dropout = dropout\n",
        "    self.cell_type = cell_type\n",
        "    self.attn_type = attn_type\n",
        "    self.init_submodules()\n",
        "\n",
        "\n",
        "  def init_submodules(self):\n",
        "    \"\"\"Initialize the submodules for encoder with attention\"\"\"\n",
        "    \n",
        "    if self.cell_type == \"VanillaRNN\":\n",
        "      net = [nn.RNNCell(input_size=self.input_size, hidden_size=self.hidden_state_size)]\n",
        "      net.extend([nn.RNNCell(input_size=self.hidden_state_size, hidden_size=self.hidden_state_size)\n",
        "                                  for _ in range(self.num_hidden_layers - 1)])\n",
        "    elif self.cell_type == \"LSTM\":\n",
        "      net = [nn.LSTMCell(input_size=self.input_size, hidden_size=self.hidden_state_size)]\n",
        "      net.extend([nn.LSTMCell(input_size=self.hidden_state_size, hidden_size=self.hidden_state_size)\n",
        "                                  for _ in range(self.num_hidden_layers - 1)])\n",
        "    \n",
        "    else:\n",
        "      net = [nn.GRUCell(input_size=self.input_size, hidden_size=self.hidden_state_size)]\n",
        "      net.extend([nn.GRUCell(input_size=self.hidden_state_size, hidden_size=self.hidden_state_size)\n",
        "                                  for _ in range(self.num_hidden_layers - 1)])\n",
        "    \n",
        "    self.rnn = nn.Sequential(*net)\n",
        "\n",
        "    self.project_to_input_dim = nn.Linear(self.hidden_state_size, self.input_size)\n",
        "\n",
        "    if self.attn_type == \"Additive\":\n",
        "      attn = [AdditiveAttention(hidden_size=self.seq_len, encoder=True) for _ in range(self.num_hidden_layers)]\n",
        "    elif self.attn_type == \"Scaled-Dot\":\n",
        "      attn = [ScaledDotAttention(hidden_size=self.seq_len, hidden_size2=self.input_size, encoder=True)]\n",
        "      attn.extend([ScaledDotAttention(hidden_size=self.seq_len, hidden_size2 = self.hidden_state_size, encoder=True) for _ in range(self.num_hidden_layers - 1)])\n",
        "    elif self.attn_type == \"Linear\":\n",
        "      attn = [LinearAttention(hidden_size=self.seq_len, encoder=True) for _ in range(self.num_hidden_layers)]\n",
        "    else:\n",
        "      attn = [DaRnnAttention(hidden_size=self.seq_len) for _ in range(self.num_hidden_layers)]\n",
        "\n",
        "    self.attn = nn.Sequential(*attn)\n",
        "\n",
        "\n",
        "  def forward(self, encoder_input):\n",
        "    \"\"\"Forward pass of encoder with attention\"\"\"\n",
        "    \n",
        "    attentions_all_layers = []\n",
        "\n",
        "    ## Iterate over all layers\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      attentions = []\n",
        "      \n",
        "      ## Get first hidden state\n",
        "      h_prev = self.get_first_hidden_state(encoder_input) ## (batch_size, input_size) or (batch_size, hidden_size) \n",
        "\n",
        "      ## Loop over the sequence\n",
        "      hiddens = []\n",
        "      for j in range(self.seq_len):\n",
        "        current_input = encoder_input[:, j, :] ## Get input at current time step (batch_size, input_size) or (batch_size, hidden_size)\n",
        "        encoder_input = encoder_input.transpose(1, 2) ## (batch_size, input_size, seq_len) or (batch_size, hidden_size, seq_len)\n",
        "\n",
        "        if self.cell_type == \"LSTM\":\n",
        "            if h_prev[0].shape[1] != encoder_input.shape[1]:\n",
        "                h_prev_pr = self.project_to_input_dim(h_prev[0]) ## (batch_size, input_size)\n",
        "            else:\n",
        "                h_prev_pr = h_prev[0]\n",
        "        else: \n",
        "            if h_prev.shape[1] != encoder_input.shape[1]:\n",
        "                h_prev_pr = self.project_to_input_dim(h_prev)\n",
        "            else:\n",
        "                h_prev_pr = h_prev\n",
        "\n",
        "        h_repeated = h_prev_pr.unsqueeze(-1).expand_as(encoder_input)\n",
        "\n",
        "        ## Pass through attention layer\n",
        "        _, attention_weights = self.attn[i](h_repeated, encoder_input, encoder_input) ## (batch_size, input_size, 1) or (batch_size, hidden_size, 1)\n",
        "        ## Pass through rnn\n",
        "        weighted_input = attention_weights.squeeze(2) * current_input\n",
        "        if i == 0:\n",
        "          h_prev = self.rnn[i](weighted_input) ## First hidden = 0\n",
        "        else:\n",
        "          h_prev = self.rnn[i](weighted_input, h_prev) ## (batch_size, hidden_size)\n",
        "        \n",
        "        ## Keep important stuff in arrays\n",
        "        attentions.append(attention_weights) ## (batch_size, hidden_size) \n",
        "        if self.cell_type == \"LSTM\":\n",
        "          hiddens.append(h_prev[0]) ## (batch_size, hidden_size) \n",
        "        else:\n",
        "          hiddens.append(h_prev)\n",
        "\n",
        "        ## Revert back to previous orientation\n",
        "        encoder_input = encoder_input.transpose(1, 2) ## (batch_size, seq_len, input_size) or (batch_size, seq_len, hidden_size)\n",
        "      \n",
        "      ## outputs of previous layer are inputs to the next layer\n",
        "      encoder_input = torch.stack(hiddens, dim=1) ## (batch_size, seq_len, hidden_size)\n",
        "      attentions = torch.cat(attentions, dim=2) ## (batch_size, hidden_size, seq_len)\n",
        "      attentions_all_layers.append(attentions)\n",
        "    \n",
        "    all_hiddens = encoder_input\n",
        "    ## attentions_all_layers is a list of num_layers tensors of size (batch_size, input or hidden_size, seq_len)\n",
        "    return all_hiddens, attentions_all_layers\n",
        "\n",
        "  def get_first_hidden_state(self, encoder_input):\n",
        "    ## Just return tensor of zeros or tuple if LSTM\n",
        "    ## Return shape: (batch_size, hidden_size)\n",
        "    htilde_tm1 = torch.zeros((encoder_input.shape[0], self.hidden_state_size), device=encoder_input.device)\n",
        "    if self.cell_type == \"LSTM\":\n",
        "      return htilde_tm1, torch.zeros_like(htilde_tm1)\n",
        "    else:\n",
        "      return htilde_tm1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKXNOEzh0NQ3"
      },
      "source": [
        "### Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "scDu6EkA0Mpf"
      },
      "source": [
        "class BidirectionalEncoderWithAttention(nn.Module):\n",
        "    def __init__(self, input_size, seq_len, num_hidden_layers, hidden_state_size, dropout=0.1, cell_type='LSTM', attn_type=None):\n",
        "      super().__init__()\n",
        "\n",
        "      self.input_size = input_size\n",
        "      self.seq_len = seq_len\n",
        "      self.num_hidden_layers = num_hidden_layers\n",
        "      self.hidden_state_size = hidden_state_size\n",
        "      self.dropout = dropout\n",
        "      self.cell_type = cell_type\n",
        "      self.attn_type = attn_type\n",
        "      self.init_submodules()\n",
        "\n",
        "    def init_submodules(self):\n",
        "      \"\"\"Initialize the submodules for encoder with attention\"\"\"\n",
        "      \n",
        "      if self.cell_type == \"VanillaRNN\":\n",
        "        net = [nn.RNNCell(input_size=self.input_size, hidden_size=self.hidden_state_size)]\n",
        "        net.extend([nn.RNNCell(input_size=self.hidden_state_size, hidden_size=self.hidden_state_size)\n",
        "                                    for _ in range(self.num_hidden_layers - 1)])\n",
        "      elif self.cell_type == \"LSTM\":\n",
        "        net = [nn.LSTMCell(input_size=self.input_size, hidden_size=self.hidden_state_size)]\n",
        "        net.extend([nn.LSTMCell(input_size=self.hidden_state_size, hidden_size=self.hidden_state_size)\n",
        "                                    for _ in range(self.num_hidden_layers - 1)])\n",
        "      \n",
        "      else:\n",
        "        net = [nn.GRUCell(input_size=self.input_size, hidden_size=self.hidden_state_size)]\n",
        "        net.extend([nn.GRUCell(input_size=self.hidden_state_size, hidden_size=self.hidden_state_size)\n",
        "                                    for _ in range(self.num_hidden_layers - 1)])\n",
        "      \n",
        "      self.rnn_forward = nn.Sequential(*net)\n",
        "      self.rnn_backward = nn.Sequential(*net.copy())\n",
        "\n",
        "      self.project_to_input_dim_forward = nn.Linear(self.hidden_state_size, self.input_size)\n",
        "      self.project_to_input_dim_backward = nn.Linear(self.hidden_state_size, self.input_size)\n",
        "\n",
        "      if self.attn_type == \"Additive\":\n",
        "        attn = [AdditiveAttention(hidden_size=self.seq_len, encoder=True) for _ in range(self.num_hidden_layers)]\n",
        "      elif self.attn_type == \"Scaled-Dot\":\n",
        "        attn = [ScaledDotAttention(hidden_size=self.seq_len, hidden_size2=self.input_size, encoder=True)]\n",
        "        attn.extend([ScaledDotAttention(hidden_size=self.seq_len, hidden_size2 = self.hidden_state_size, encoder=True) for _ in range(self.num_hidden_layers - 1)])\n",
        "      elif self.attn_type == \"Linear\":\n",
        "        attn = [LinearAttention(hidden_size=self.seq_len, encoder=True) for _ in range(self.num_hidden_layers)]\n",
        "      else:\n",
        "        attn = [DaRnnAttention(hidden_size=self.seq_len) for _ in range(self.num_hidden_layers)]\n",
        "\n",
        "      self.attn_forward = nn.Sequential(*attn)\n",
        "      self.attn_backward = nn.Sequential(*attn.copy()) \n",
        "      \n",
        "      self.forward_attn_weights = None\n",
        "      self.backward_attn_weights = None\n",
        "\n",
        "    def forward(self, data):\n",
        "      \"\"\"Forward pass of data in encoder with input attention\"\"\"\n",
        "\n",
        "      forward_output, self.forward_attn_weights = self.forward_pass(data)\n",
        "      backward_output, self.backward_attn_weights = self.backward_pass(data)\n",
        "      output = torch.cat([forward_output, backward_output], dim=2)\n",
        "      \n",
        "      return output, (self.forward_attn_weights, self.backward_attn_weights)\n",
        "\n",
        "    def forward_pass(self, data, direction='Forward'):\n",
        "      \"\"\"Forward pass of encoder with attention in the forward direction\"\"\"\n",
        "      \n",
        "      attentions_all_layers = []\n",
        "\n",
        "      if direction == 'Forward':\n",
        "          attn_net = self.attn_forward\n",
        "          rnn = self.rnn_forward\n",
        "          project_to_input_dim = self.project_to_input_dim_forward\n",
        "      else:\n",
        "          attn_net = self.attn_backward\n",
        "          rnn = self.rnn_backward\n",
        "          project_to_input_dim = self.project_to_input_dim_backward\n",
        "\n",
        "      ## Iterate over all layers\n",
        "      for i in range(self.num_hidden_layers):\n",
        "        attentions = []\n",
        "        \n",
        "        ## Get first hidden state\n",
        "        h_prev = self.get_first_hidden_state(data) ## (batch_size, input_size) or (batch_size, hidden_size) \n",
        "\n",
        "        ## Loop over the sequence\n",
        "        hiddens = []\n",
        "        for j in range(self.seq_len):\n",
        "          current_input = data[:, j, :] ## Get input at current time step (batch_size, input_size) or (batch_size, hidden_size)\n",
        "          data = data.transpose(1, 2) ## (batch_size, input_size, seq_len) or (batch_size, hidden_size, seq_len)\n",
        "\n",
        "          if self.cell_type == \"LSTM\":\n",
        "              if h_prev[0].shape[1] != data.shape[1]:\n",
        "                  h_prev_pr = project_to_input_dim(h_prev[0]) ## (batch_size, input_size)\n",
        "              else:\n",
        "                  h_prev_pr = h_prev[0]\n",
        "          else:\n",
        "              if h_prev.shape[1] != data.shape[1]:\n",
        "                  h_prev_pr = project_to_input_dim(h_prev)\n",
        "              else:\n",
        "                  h_prev_pr = h_prev\n",
        "\n",
        "          h_repeated = h_prev_pr.unsqueeze(-1).expand_as(data)  ## (batch_size, input_size, seq_len)\n",
        "\n",
        "          ## Pass through attention layer\n",
        "          _, attention_weights = attn_net[i](h_repeated, data, data) ## (batch_size, input_size, 1) or (batch_size, hidden_size, 1)\n",
        "          ## Pass through rnn\n",
        "          weighted_input = attention_weights.squeeze(2) * current_input\n",
        "          h_prev = rnn[i](weighted_input, h_prev) ## (batch_size, hidden_size)\n",
        "          \n",
        "          ## Keep important stuff in arrays\n",
        "          attentions.append(attention_weights) ## (batch_size, hidden_size) \n",
        "          if self.cell_type == \"LSTM\":\n",
        "            hiddens.append(h_prev[0]) ## (batch_size, hidden_size) \n",
        "          else:\n",
        "            hiddens.append(h_prev)\n",
        "\n",
        "          ## Revert back to previous orientation\n",
        "          data = data.transpose(1, 2) ## (batch_size, seq_len, input_size) or (batch_size, seq_len, hidden_size)\n",
        "        \n",
        "        ## outputs of previous layer are inputs to the next layer\n",
        "        data = torch.stack(hiddens, dim=1) ## (batch_size, seq_len, hidden_size)\n",
        "        attentions = torch.cat(attentions, dim=2) ## (batch_size, hidden_size, seq_len)\n",
        "        attentions_all_layers.append(attentions)\n",
        "      \n",
        "      final_hiddens = data\n",
        "      ## attentions_all_layers is a list of num_layers tensors of size (batch_size, input or hidden_size, seq_len)\n",
        "      return final_hiddens, attentions_all_layers\n",
        "    \n",
        "    def backward_pass(self, data):\n",
        "        \"\"\"RNN in the reverse direction\"\"\"\n",
        "        ## Reverse the input data along the sequence dimension\n",
        "        data = data.flip(dims=(1,))\n",
        "\n",
        "        return self.forward_pass(data, direction='Backward')\n",
        "\n",
        "    def get_first_hidden_state(self, encoder_input):\n",
        "      ## Just return tensor of zeros or tuple if LSTM\n",
        "      ## Return shape: (batch_size, hidden_size)\n",
        "      htilde_tm1 = torch.zeros((encoder_input.shape[0], self.hidden_state_size), device=encoder_input.device)\n",
        "      if self.cell_type == \"LSTM\":\n",
        "        return htilde_tm1, torch.zeros_like(htilde_tm1)\n",
        "      else:\n",
        "        return htilde_tm1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZxxn1rNMEid"
      },
      "source": [
        "## Decoder with Attention\n",
        "Stacked RNN with attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_XxEdhyaMDoL"
      },
      "source": [
        "class DecoderWithAttention(nn.Module):\n",
        "  def __init__(self, output_size, seq_len, num_hidden_layers, hidden_state_size, cell_type='LSTM', attn_type='Additive'):\n",
        "    super().__init__()\n",
        "\n",
        "    self.output_size = output_size\n",
        "    self.seq_len = seq_len\n",
        "    self.num_hidden_layers = num_hidden_layers\n",
        "    self.hidden_state_size = hidden_state_size \n",
        "    self.cell_type = cell_type\n",
        "    self.attn_type = attn_type\n",
        "    self.init_submodules()\n",
        "\n",
        "\n",
        "  def init_submodules(self):\n",
        "    \"\"\"Initialize the submodules for decoder with attention\"\"\"\n",
        "\n",
        "    if self.cell_type == \"VanillaRNN\":\n",
        "      net = [nn.RNNCell(input_size=self.output_size + self.hidden_state_size, hidden_size=self.hidden_state_size)]\n",
        "      net.extend([nn.RNNCell(input_size=self.hidden_state_size*2, hidden_size=self.hidden_state_size)\n",
        "                                  for _ in range(self.num_hidden_layers - 1)])\n",
        "    elif self.cell_type == \"LSTM\":\n",
        "      net = [nn.LSTMCell(input_size=self.output_size + self.hidden_state_size, hidden_size=self.hidden_state_size)]\n",
        "      net.extend([nn.LSTMCell(input_size=self.hidden_state_size * 2, hidden_size=self.hidden_state_size)\n",
        "                                  for _ in range(self.num_hidden_layers - 1)])\n",
        "    else:\n",
        "      net = [nn.GRUCell(input_size=self.output_size + self.hidden_state_size, hidden_size=self.hidden_state_size)]\n",
        "      net.extend([nn.GRUCell(input_size=self.hidden_state_size * 2, hidden_size=self.hidden_state_size)\n",
        "                                  for _ in range(self.num_hidden_layers - 1)])\n",
        "    \n",
        "    self.rnn = nn.Sequential(*net)\n",
        "\n",
        "    if self.attn_type == \"Additive\":\n",
        "      attn = [AdditiveAttention(hidden_size=self.hidden_state_size) for _ in range(self.num_hidden_layers)]\n",
        "    elif self.attn_type == \"Scaled-Dot\":\n",
        "      attn = [ScaledDotAttention(hidden_size=self.hidden_state_size) for _ in range(self.num_hidden_layers)]\n",
        "    elif self.attn_type == \"Linear\":\n",
        "      attn = [LinearAttention(hidden_size=self.hidden_state_size) for _ in range(self.num_hidden_layers)]\n",
        "    else:\n",
        "      attn = [DaRnnAttention(hidden_size=self.hidden_state_size) for _ in range(self.num_hidden_layers)]\n",
        "  \n",
        "    self.attn = nn.Sequential(*attn)\n",
        "\n",
        "  def forward(self, encoder_output, decoder_input):\n",
        "    htilde_tm1 = self.get_first_hidden_state(encoder_output)\n",
        "    return self.forward_pass(encoder_output, htilde_tm1, decoder_input)\n",
        "\n",
        "\n",
        "  def forward_pass(self, encoder_output, htilde_tm1, decoder_input):\n",
        "    \"\"\"Forward pass of decoder with attention.\n",
        "    Params:\n",
        "    encoder_output: output of the encoder (batch_size, seq_len, hidden_size)\n",
        "                    Note: hidden_size = encoder_hidden_size * 2 (if encoder is bidirection; otherwise, equal)\n",
        "    htilde_tm1: first hidden state (tensor or tuple of 2 tensors) (batch_size, hidden_size)\n",
        "    decoder_input: input to the decoder (batch_size, seq_len, output_size)\n",
        "    \"\"\"\n",
        "\n",
        "    attentions_all_layers = []\n",
        "\n",
        "    ## Iterate over all layers\n",
        "    for i in range(self.num_hidden_layers):\n",
        "      attentions = []\n",
        "      ## Get first hidden state for the layer\n",
        "      h_prev = htilde_tm1\n",
        "\n",
        "      ## Loop over the sequence\n",
        "      hiddens = []\n",
        "      for j in range(self.seq_len):\n",
        "        current_input = decoder_input[:, j, :] ## Get input at current time step (batch_size, output_size)\n",
        "        ## Pass through attention layer\n",
        "        if self.cell_type == \"LSTM\":\n",
        "          context, attention_weights = self.attn[i](h_prev[0], encoder_output, encoder_output) ## (batch_size, 1, hidden_size)\n",
        "        else:\n",
        "          context, attention_weights = self.attn[i](h_prev, encoder_output, encoder_output)\n",
        "        \n",
        "        ## Concatenate with the current inputs\n",
        "        input_and_context = torch.cat([current_input, context.squeeze(1)], dim=1) ## (batch_size, hidden_size + output_size)\n",
        "        h_prev = self.rnn[i](input_and_context, h_prev) ## (batch_size, hidden_size)\n",
        "\n",
        "        attentions.append(attention_weights) ## (batch_size, seq_len, 1)\n",
        "        if self.cell_type == \"LSTM\":\n",
        "          hiddens.append(h_prev[0]) ## (batch_size, hidden_size)\n",
        "        else:\n",
        "          hiddens.append(h_prev)\n",
        "      \n",
        "      ## outputs of previous layer are inputs to the next layer\n",
        "      decoder_input = torch.stack(hiddens, dim=1) ## (batch_size, seq_len, hidden_size)\n",
        "      attentions = torch.cat(attentions, dim=2) ## (batch_size, seq_len, seq_len)\n",
        "      attentions_all_layers.append(attentions)\n",
        "    \n",
        "    final_hidden = decoder_input[:, -1, :]\n",
        "    all_hiddens = decoder_input\n",
        "    attentions_all_layers = torch.stack(attentions_all_layers, dim=0) ## (num_layers, batch_size, seq_len, seq_len)\n",
        "    return all_hiddens, final_hidden, attentions_all_layers\n",
        "\n",
        "  def get_first_hidden_state(self, encoder_output):\n",
        "    ## Just return tensor of zeros\n",
        "    ## Return shape: (batch_size, hidden_size)\n",
        "    batch_size = encoder_output.shape[0]\n",
        "    htilde_tm1 = torch.zeros((batch_size, self.hidden_state_size), device=encoder_output.device)\n",
        "    if self.cell_type == 'LSTM':\n",
        "        htilde_tm1 = htilde_tm1, torch.zeros_like(htilde_tm1)\n",
        "    return htilde_tm1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQiMROdoOXbS"
      },
      "source": [
        "# Attention Layers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wlDSXocbOzRe"
      },
      "source": [
        "## Additive Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcNdAwbIOWDw"
      },
      "source": [
        "class AdditiveAttention(nn.Module):\n",
        "  def __init__(self, hidden_size, encoder=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.encoder = encoder\n",
        "    self.init_submodules()\n",
        "\n",
        "  def init_submodules(self):\n",
        "    self.attention = nn.Sequential(nn.Linear(2 * self.hidden_size, self.hidden_size),\n",
        "                                   nn.ReLU(),\n",
        "                                   nn.Linear(self.hidden_size, 1))\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "  def forward(self, queries, keys, values):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      queries: current decoder hidden state (batch_size, hidden_size)\n",
        "        if self.encoder: previous encoder hidden state (batch_size, input_size/hidden_size, seq_len)\n",
        "      keys: all encoder hidden states  (batch_size, seq_len, hidden_size)\n",
        "        if self.encoder: all inputs (batch_size, input_size/hidden_size, seq_len)\n",
        "      values: all encoder hidden states (batch_size, seq_len, hidden_size)\n",
        "        if self.encoder: all inputs (batch_size, input_size/hidden_size, seq_len)\n",
        "    Outputs:\n",
        "      context: weighted average of the keys/values (for decoder only) (batch_size, 1, hidden_size)\n",
        "      attn_weights: calculated weights (batch_size, seq_len, 1)\n",
        "        if self.encoder: (batch_size, input_size/hidden_size, 1)\n",
        "    \"\"\"\n",
        "    batch_size = keys.size(0)\n",
        "    if not self.encoder:\n",
        "      expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys)\n",
        "    else:\n",
        "      expanded_queries = queries\n",
        "    concat_input = torch.cat((expanded_queries, keys), dim=2)\n",
        "    unnormalized_attn_weights = self.attention(concat_input)\n",
        "    attn_weights = self.softmax(unnormalized_attn_weights)  # normalize weights\n",
        "    context = torch.bmm(attn_weights.transpose(2,1), values)\n",
        "\n",
        "    return context, attn_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4B6H3bVvyw9u"
      },
      "source": [
        "## (Modified) Scaled Dot Product Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RnC7NWK8yzcl"
      },
      "source": [
        "class ScaledDotAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, hidden_size2=0, encoder=False):\n",
        "        \"\"\"Note that if encoder = True, hidden_size = seq_len, hidden_size2 is actual hidden size of encoder\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.encoder = encoder\n",
        "        self.hidden_size = hidden_size\n",
        "        if self.encoder:\n",
        "            self.hidden_size2 = hidden_size2\n",
        "        else:\n",
        "            self.hidden_size2 = hidden_size\n",
        "        self.init_submodules()\n",
        "\n",
        "    def init_submodules(self):\n",
        "        self.Q = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.K = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.V = nn.Linear(self.hidden_size, self.hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.scaling_factor = torch.rsqrt(torch.tensor(self.hidden_size, dtype= torch.float))\n",
        "\n",
        "        self.lin = nn.Linear(self.hidden_size2, 1)\n",
        "    \n",
        "    def forward(self, queries, keys, values):\n",
        "      \"\"\"\n",
        "      Inputs:\n",
        "        queries: current decoder hidden state (batch_size, hidden_size)\n",
        "          if self.encoder: previous encoder hidden state (batch_size, input_size/hidden_size, seq_len)\n",
        "        keys: all encoder hidden states  (batch_size, seq_len, hidden_size)\n",
        "          if self.encoder: all inputs (batch_size, input_size/hidden_size, seq_len)\n",
        "        values: all encoder hidden states (batch_size, seq_len, hidden_size)\n",
        "          if self.encoder: all inputs (batch_size, input_size/hidden_size, seq_len)\n",
        "      Outputs:\n",
        "        context: weighted average of the keys/values (for decoder only) (batch_size, 1, hidden_size)\n",
        "        attn_weights: calculated weights (batch_size, seq_len, 1)\n",
        "          if self.encoder: (batch_size, input_size/hidden_size, 1)\n",
        "      \"\"\"\n",
        "      batch_size = queries.shape[0]\n",
        "      if not self.encoder:\n",
        "        q = self.Q(queries).view(batch_size, -1, self.hidden_size).transpose(2, 1)\n",
        "      else:\n",
        "        q = queries.transpose(2, 1)\n",
        "      k = self.K(keys)\n",
        "      v = self.V(values)\n",
        "      unnormalized_attention = (k @ q) * self.scaling_factor # (B, S, H) x (B, H, k) = (B, S, k)\n",
        "      if self.encoder:\n",
        "        unnormalized_attention = self.lin(unnormalized_attention) # (B, S, 1)\n",
        "      attention_weights = self.softmax(unnormalized_attention) # (B, S, k) or (B, S, 1) if encoder\n",
        "      if self.encoder:\n",
        "        context = None\n",
        "      else:\n",
        "        context = (attention_weights.transpose(2,1) @ v).squeeze() # (B, k, S) x (B, S, H) = (B, k, H)\n",
        "      return context, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aILVjekX1Vch"
      },
      "source": [
        "## Linear Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb8NNYss1bgV"
      },
      "source": [
        "class LinearAttention(nn.Module):\n",
        "    def __init__(self, hidden_size, encoder=False):\n",
        "        super().__init__()\n",
        "\n",
        "        self.hidden_size = hidden_size\n",
        "        self.encoder = encoder\n",
        "        self.init_submodules()\n",
        "\n",
        "\n",
        "    def init_submodules(self):\n",
        "        self.attn = nn.Linear(self.hidden_size * 2, 1)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, queries, keys, values):\n",
        "      \"\"\"\n",
        "      Inputs:\n",
        "        queries: current decoder hidden state (batch_size, hidden_size)\n",
        "          if self.encoder: previous encoder hidden state (batch_size, input_size/hidden_size, seq_len)\n",
        "        keys: all encoder hidden states  (batch_size, seq_len, hidden_size)\n",
        "          if self.encoder: all inputs (batch_size, input_size/hidden_size, seq_len)\n",
        "        values: all encoder hidden states (batch_size, seq_len, hidden_size)\n",
        "          if self.encoder: all inputs (batch_size, input_size/hidden_size, seq_len)\n",
        "      Outputs:\n",
        "        context: weighted average of the keys/values (for decoder only) (batch_size, 1, hidden_size)\n",
        "        attn_weights: calculated weights (batch_size, seq_len, 1)\n",
        "          if self.encoder: (batch_size, input_size/hidden_size, 1)\n",
        "      \"\"\"\n",
        "      if not self.encoder:\n",
        "          queries = queries.view((-1, 1, self.hidden_size)).expand_as(keys)\n",
        "      cat = torch.cat([queries, keys], dim=2) ## (batch_size, seq_len, hidden_size * 2)\n",
        "      unnormalized_attention = self.attn(cat) ## (batch_size, seq_len, 1)\n",
        "      attention_weights = self.softmax(unnormalized_attention)\n",
        "      context = None\n",
        "      if not self.encoder:\n",
        "          context = (attention_weights.transpose(2,1) @ values).squeeze() # (B, 1, S) x (B, S, H) = (B, 1, H)\n",
        "      return context, attention_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iVZIA1tA4ODd"
      },
      "source": [
        "## DA-RNN Attention\n",
        "Note: same thing as Additive Attention, but with Tanh activation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G2gw02B14TDH"
      },
      "source": [
        "class DaRnnAttention(nn.Module):\n",
        "  def __init__(self, hidden_size, encoder=False):\n",
        "    super().__init__()\n",
        "\n",
        "    self.hidden_size = hidden_size\n",
        "    self.encoder = encoder\n",
        "    self.init_submodules()\n",
        "\n",
        "\n",
        "  def init_submodules(self):\n",
        "    self.attention = nn.Sequential(nn.Linear(2*self.hidden_size, self.hidden_size),\n",
        "                                   nn.Tanh(),\n",
        "                                   nn.Linear(self.hidden_size, 1))\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=1)\n",
        "\n",
        "\n",
        "  def forward(self, queries, keys, values):\n",
        "    \"\"\"\n",
        "    Inputs:\n",
        "      queries: current decoder hidden state (batch_size, hidden_size)\n",
        "        if self.encoder: previous encoder hidden state (batch_size, input_size/hidden_size, seq_len)\n",
        "      keys: all encoder hidden states  (batch_size, seq_len, hidden_size)\n",
        "        if self.encoder: all inputs (batch_size, input_size/hidden_size, seq_len)\n",
        "      values: all encoder hidden states (batch_size, seq_len, hidden_size)\n",
        "        if self.encoder: all inputs (batch_size, input_size/hidden_size, seq_len)\n",
        "    Outputs:\n",
        "      context: weighted average of the keys/values (for decoder only) (batch_size, 1, hidden_size)\n",
        "      attn_weights: calculated weights (batch_size, seq_len, 1)\n",
        "        if self.encoder: (batch_size, input_size/hidden_size, 1)\n",
        "    \"\"\"\n",
        "    batch_size = keys.size(0)\n",
        "    if not self.encoder:\n",
        "      expanded_queries = queries.view(batch_size, -1, self.hidden_size).expand_as(keys)\n",
        "    else:\n",
        "      expanded_queries = queries\n",
        "    concat_input = torch.cat((expanded_queries, keys), dim=2)\n",
        "    unnormalized_attn_weights = self.attention(concat_input)\n",
        "    attn_weights = self.softmax(unnormalized_attn_weights)  # normalize weights\n",
        "    context = torch.bmm(attn_weights.transpose(2,1), values)\n",
        "\n",
        "    return context, attn_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9JHqsS4SETV"
      },
      "source": [
        "# Training and Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bfNHODj3SGRg"
      },
      "source": [
        "## Training and Evaluation Code \n",
        "## Note: for optimization, we use MSE loss\n",
        "\n",
        "def compute_loss(output, target, loss_fn):\n",
        "  \"\"\"\n",
        "  Arguments:\n",
        "    model: type of model being used\n",
        "    data: input data we compute loss on. Make sure to split data properly depending on the type of model being used 'R' vs 'ED'\n",
        "      For 'R' we just need the main features (X_tilde and y), for 'ED' we set X for encoder_input, X_tilde for decoder_input and y as expected output\n",
        "      Maybe for 'R' we use all features?\n",
        "    loss_fn: the loss function being used \n",
        "  \"\"\"\n",
        "  loss = loss_fn(output, target) \n",
        "\n",
        "  return loss\n",
        "\n",
        "\n",
        "def training_loop(model, dataloader, optimizer, device):\n",
        "    \"\"\"Training loop\n",
        "    Params:\n",
        "    model: the model\n",
        "    dataloader: the training data\n",
        "    Optimizer: optimizer for gradient descent\n",
        "    device: cpu or cuda\n",
        "    \"\"\"\n",
        "    loss_fn = nn.MSELoss().to(device) ## average MSE loss\n",
        "    total_train_loss = 0\n",
        "    total_train_sequences = 0\n",
        "\n",
        "    for i, data in enumerate(dataloader): \n",
        "        # print(\"Batch {}\".format(i))\n",
        "        X_train, X_tilde_train, y_train = data\n",
        "\n",
        "        ## Send to device\n",
        "        X_train = X_train.to(device=device) \n",
        "        if X_tilde_train is not None:\n",
        "            X_tilde_train = X_tilde_train.to(device=device) \n",
        "        y_train = y_train.to(device=device)\n",
        "\n",
        "        # Zero out gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Run input through model\n",
        "        output = model(X_train, X_tilde_train)\n",
        "\n",
        "        # Calculate batched training loss\n",
        "        loss = compute_loss(output, y_train, loss_fn)\n",
        "        num_sequences = X_train.shape[0]\n",
        "        total_train_loss += loss * num_sequences\n",
        "        total_train_sequences += num_sequences\n",
        "        \n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "        # Update gradients\n",
        "        optimizer.step()\n",
        "\n",
        "        del X_train, X_tilde_train, y_train, loss ## To save space\n",
        "\n",
        "    return total_train_loss / total_train_sequences\n",
        "\n",
        "\n",
        "def train(X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, opts):\n",
        "    \"\"\"Train model\n",
        "    Params:\n",
        "    X_train: input to encoder\n",
        "    X_tilde_train: input to decoder\n",
        "    y_train: training labels\n",
        "    X_val: validation input to encoder\n",
        "    X_tilde_val: validation input to decoder\n",
        "    y_val: training labels\n",
        "    opts: dictionary of hyperparameters (AttrDict)\n",
        "    \"\"\"\n",
        "\n",
        "    loss_fn = nn.MSELoss().to(opts.device)\n",
        "    dataloader = CovidDataLoader(X_train, X_tilde_train, y_train, opts.batch_size)\n",
        "    if opts.model == 'ED':\n",
        "      model = EncoderDecoder(opts.encoder_class, opts.decoder_class, opts.input_size, opts.seq_len, opts.encoder_num_hidden_layers, \n",
        "               opts.hidden_state_size, opts.output_size, opts.dropout, opts.cell_type,\n",
        "               opts.enc_attn_type, opts.decoder_num_hidden_layers, opts.dec_attn_type, opts.bidirectional, opts.temporal_attn)\n",
        "    else:\n",
        "      model = MyRNN(opts.rnn_class, opts.input_size, opts.hidden_state_size, opts.output_size, opts.num_layers, opts.seq_len, opts.dropout, \n",
        "                    opts.bidirectional, opts.cell_type, opts.rnn_attn_type, opts.temporal_attn)\n",
        "\n",
        "    if opts.device == 'cuda':\n",
        "        model.cuda()\n",
        "\n",
        "    optimizer = optim.Adam(list(model.parameters()), lr=opts.lr) \n",
        "\n",
        "    losses = {'epoch': [], 'train': [], 'val': []}\n",
        "    X_val = X_val.type(torch.FloatTensor).to(device=opts.device)\n",
        "    if X_tilde_val is not None:\n",
        "        X_tilde_val = X_tilde_val.type(torch.FloatTensor).to(device=opts.device)\n",
        "    y_val = y_val.type(torch.FloatTensor).to(device=opts.device)\n",
        "\n",
        "    mae_fn = nn.L1Loss().to(opts.device)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    early_stopping_counter = 0\n",
        "    for epoch in range(opts.num_epochs):\n",
        "        ## Update params\n",
        "        avg_train_loss = training_loop(model, dataloader, optimizer, opts.device)\n",
        "\n",
        "        ## Calculate validation loss\n",
        "        output = model(X_val, X_tilde_val) \n",
        "        avg_val_loss = compute_loss(output, y_val, loss_fn)\n",
        "        avg_mae_loss = mae_fn(output, y_val)\n",
        "\n",
        "        if avg_val_loss < best_val_loss:\n",
        "            checkpoint(model, opts)\n",
        "            best_val_loss = avg_val_loss\n",
        "            early_stopping_counter = 0\n",
        "        else:\n",
        "            early_stopping_counter += 1\n",
        "        \n",
        "        if early_stopping_counter > opts.early_stopping_patience:\n",
        "            # print(\"Validation loss has not improved in {} epochs, stopping early\".format(opts.early_stopping_patience))\n",
        "            # print(\"Obtained lowest validation loss of: {}\".format(best_val_loss))\n",
        "            return losses\n",
        "\n",
        "        losses['epoch'].append(epoch)\n",
        "        losses['train'].append(avg_train_loss)\n",
        "        losses['val'].append(avg_val_loss)\n",
        "        \n",
        "        if epoch > 0 and avg_val_loss > losses['val'][-2]: ## lr decay\n",
        "                optimizer.param_groups[0]['lr'] *= opts.lr_decay\n",
        "\n",
        "        if opts.print:\n",
        "            print(\"Epoch {} | Train loss: {:.3f} | Val loss: {:.3f} | Val MAE: {:.3f}\".format(epoch, avg_train_loss, avg_val_loss, avg_mae_loss))\n",
        "    return losses\n",
        "\n",
        "def checkpoint(model, opts):\n",
        "    \"\"\"Saves the current model.\n",
        "    \"\"\"\n",
        "    with open('{}.pt'.format(opts.name), 'wb') as f:\n",
        "        torch.save(model, f)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCFA6sZMvHQX"
      },
      "source": [
        "# Visualize Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2kRoI3ZDvJY7"
      },
      "source": [
        "def get_condition_window(opts, num=None):\n",
        "    \"\"\"Get the dates for the condition window\"\"\"\n",
        "    if num is None:\n",
        "        pred_date_idx = np.where(ALL_DATES == opts.pred_date)[0][0]\n",
        "    else:\n",
        "        pred_date_idx = np.where(ALL_DATES == opts.pred_date[num])[0][0]\n",
        "    \n",
        "    cond_start = pred_date_idx - opts.k - opts.t + 1\n",
        "    condition_window = ALL_DATES[cond_start : cond_start + opts.k]\n",
        "\n",
        "    return condition_window\n",
        "\n",
        "def visualize_attention_helper(attention, opts, fig, ax, encoder=False, forward=True): \n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "    attention: encoder or decoder attention matrices\n",
        "    opts: plotting options to customize output figure and subplots\n",
        "    encoder: whether visualization is for encoder or decoder\n",
        "  \"\"\" \n",
        "  if not encoder:\n",
        "      batch_size, hidden_size, seq_len = attention.shape  # hidden_size = seq_len if decoder, otherwise for encoder\n",
        "  else:\n",
        "      batch_size, hidden_size, seq_len = attention[0].shape\n",
        "  \n",
        "  if isinstance(opts.plot_num, int):\n",
        "      assert opts.plot_num <= batch_size - 1\n",
        "  else:\n",
        "      assert opts.plot_num[-1] <= batch_size - 1\n",
        "\n",
        "  if not encoder:\n",
        "      attention = attention.cpu().detach().numpy() \n",
        "  else:\n",
        "      attention = attention[0].cpu().detach().numpy()\n",
        "  \n",
        "  if isinstance(opts.plot_num, int):\n",
        "      opts.plot_num = np.array([opts.plot_num])\n",
        "      opts.pred_date = np.arrage([opts.pred_date])\n",
        "\n",
        "  for i, plot_num in enumerate(opts.plot_num):\n",
        "      layer_attn = attention[plot_num, :, :]  # attention of single layer (seq_len, hidden_size or seq_len)\n",
        "\n",
        "      cax = ax[i].matshow(layer_attn, vmin=0., vmax=1.)\n",
        "\n",
        "      condition_window = get_condition_window(opts, num=i)\n",
        "      if encoder:\n",
        "          ax[i].set_yticklabels([''] + opts.in_features)\n",
        "\n",
        "          if forward:\n",
        "              x_label = \"forward input, {}\".format(opts.pred_date[i])\n",
        "          else:\n",
        "              x_label = \"backward input, {}\".format(opts.pred_date[i])\n",
        "\n",
        "          ax[i].set_xticklabels([''] + list(condition_window), rotation=90)\n",
        "      elif opts.plt_temporal and not encoder:\n",
        "          ax[i].set_yticklabels([''] + list(condition_window))\n",
        "          ax[i].set_xticklabels([''])\n",
        "          x_label = \"temporal, {}\".format(opts.pred_date[i])\n",
        "\n",
        "      ax[i].xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      ax[i].yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
        "      \n",
        "      ax[i].grid('off')\n",
        "      ax[i].set_xlabel(x_label)\n",
        "\n",
        "\n",
        "def visualize_attention(model, plt_opts):\n",
        "  \"\"\"\n",
        "  Inputs:\n",
        "    model: final model after training\n",
        "    plt_opts: plotting options to customize output figure and subplots\n",
        "  \n",
        "  Shows attention for encoder and decoder if it exists\n",
        "  \"\"\"\n",
        "  \n",
        "  if isinstance(plt_opts.plot_num, np.ndarray):\n",
        "      ncols = plt_opts.plot_num.shape[0]\n",
        "  else:\n",
        "      ncols = 1\n",
        "\n",
        "  if plt_opts.model_type == 'ed':\n",
        "      input_attn = model.input_attn_weights\n",
        "      if input_attn is not None:\n",
        "          forward_attn, backward_attn = input_attn\n",
        "      else:\n",
        "          forward_attn, backward_attn = None, None\n",
        "      temporal_attn = model.temporal_attn_weights \n",
        "  else:\n",
        "      forward_attn, backward_attn, temporal_attn = model.forward_attn_weights, model.backward_attn_weights, model.temporal_attn_weights\n",
        "\n",
        "  ## Visualize input attention weights if they are not None\n",
        "  if forward_attn is not None and plt_opts.plt_forward:\n",
        "      fig, ax = plt.subplots(nrows=1, ncols=ncols, figsize=(plt_opts.height, plt_opts.width), constrained_layout=True) \n",
        "      visualize_attention_helper(forward_attn, plt_opts, fig, ax, encoder=True)\n",
        "      plt.savefig(\"experiment3/{}_{}.pdf\".format(plt_opts.model_name, \"forward\"))\n",
        "      plt.show()\n",
        "\n",
        "  if backward_attn is not None and plt_opts.plt_backward:\n",
        "      fig, ax = plt.subplots(nrows=1, ncols=ncols, figsize=(plt_opts.height, plt_opts.width), constrained_layout=True) \n",
        "      visualize_attention_helper(backward_attn, plt_opts, fig, ax, encoder=True, forward=False)\n",
        "      plt.savefig(\"experiment3/{}_{}.pdf\".format(plt_opts.model_name, \"backward\"))\n",
        "      plt.show()\n",
        "  # visualize the temporal attention weights if it is not None\n",
        "  if temporal_attn is not None and plt_opts.plt_temporal:\n",
        "      fig, ax = plt.subplots(nrows=1, ncols=ncols, figsize=(plt_opts.height, plt_opts.width), constrained_layout=True) \n",
        "      visualize_attention_helper(temporal_attn, plt_opts, fig, ax, encoder=False)\n",
        "      plt.savefig(\"experiment4/{}_{}.pdf\".format(plt_opts.model_name, \"temporal\"))\n",
        "      plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXlCFOkzQwfZ"
      },
      "source": [
        "# Run Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtD3r8kIu_4P"
      },
      "source": [
        "## Run RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3MH5NlCtvD2m"
      },
      "source": [
        "## Loop over every subset of in_features\n",
        "opts = AttrDict()\n",
        "args_dict = {       'name': 'rnn_wo_attn',\n",
        "                    'model':'R', ## R, ED\n",
        "                    'rnn_class': RNNWithoutAttention,\n",
        "                    'rnn_attn_type': None,\n",
        "                    'temporal_attn': True,\n",
        "                    'input_size': 15, ## Random -- to be set in the loop\n",
        "                    'output_size': 3, ## Random\n",
        "                    'seq_len': 5, ## Random\n",
        "                    'hidden_state_size': 16,\n",
        "                    'num_layers': 1, \n",
        "                    'bidirectional': True, \n",
        "                    'dropout': 0.,\n",
        "                    'cell_type': 'LSTM', ## VanillaRNN, LSTM, GRU\n",
        "                    'device': 'cpu', ## 'cuda', 'cpu'\n",
        "                    'num_epochs': 70, \n",
        "                    'rnn_attn_type': None,\n",
        "                    'lr':1e-3,\n",
        "                    'lr_decay':0.999,\n",
        "                    'batch_size': 32,\n",
        "                    'early_stopping_patience': 10,\n",
        "                    'momentum': 0.9,\n",
        "                    'print': True\n",
        "      }\n",
        "opts.update(args_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mAiTonxgnh7e"
      },
      "source": [
        "out_features = ['Deaths']\n",
        "in_features = BASIC_FEATURES + TEST_FEATURES + HOSPITAL_FEATURES + LTC_FEATURES\n",
        "X, X_tilde, y, y_cumul, dates, in_features = cluster_data(df, k=5, t=7, separate=True, in_features=in_features, out_features=out_features, moving_average=True, m=3)\n",
        "X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, X_test, X_tilde_test, y_test = train_val_test_split(X, X_tilde, y, dates)\n",
        "\n",
        "opts.input_size = X_train.shape[2]\n",
        "opts.output_size = y_train.shape[1]\n",
        "opts.seq_len = X_train.shape[1]\n",
        "losses = train(X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dw_wt5R-z-XW"
      },
      "source": [
        "trained_rnn = torch.load('rnn_wo_attn.pt')\n",
        "args_dict = opts\n",
        "pred = trained_rnn(X_train.to(args_dict.device), X_tilde_train.to(args_dict.device))\n",
        "pred = pred.cpu().detach().numpy()\n",
        "plt.figure()\n",
        "plt.plot(y_train, label=\"truth\")\n",
        "plt.plot(pred, label=\"pred\")\n",
        "plt.title(\"Cumulative Training Deaths\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "X_train_last_day_of_condition = X_tilde_train[:, -1, 0].cpu().detach().numpy()\n",
        "pred_diff = pred[:, 0] - X_train_last_day_of_condition\n",
        "y_train_diff = y_train[:, 0] - X_train_last_day_of_condition\n",
        "plt.figure()\n",
        "plt.plot(y_train_diff, label=\"truth\")\n",
        "plt.plot(pred_diff, label=\"pred\")\n",
        "plt.title(\"New Training Deaths\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "pred = trained_rnn(X_val.to(args_dict.device), X_tilde_val.to(args_dict.device))\n",
        "pred = pred.cpu().detach().numpy()\n",
        "plt.figure()\n",
        "plt.plot(y_val, label=\"truth\")\n",
        "plt.plot(pred, label=\"pred\")\n",
        "plt.title(\"Cumulative Validation Deaths\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "X_val_last_day_of_condition = X_tilde_val[:, -1, 0].cpu().detach().numpy()\n",
        "pred_diff = pred[:, 0] - X_val_last_day_of_condition\n",
        "y_val_diff = y_val[:, 0] - X_val_last_day_of_condition\n",
        "plt.figure()\n",
        "plt.plot(y_val_diff, label=\"truth\")\n",
        "plt.plot(pred_diff, label=\"pred\")\n",
        "plt.title(\"New Training Deaths\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rji3v_U6gyf-"
      },
      "source": [
        "## Run RNN with Input Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9IaKasX9g3No"
      },
      "source": [
        "opts = AttrDict()\n",
        "args_dict = {       'name': 'rnn_w_input_attn',\n",
        "                    'model':'R', ## R, ED\n",
        "                    'rnn_class': RNNWithAttention, ## RNNWithoutAttention\n",
        "                    'temporal_attn': True,\n",
        "                    'input_size': 15, ## Random -- to be set in the loop\n",
        "                    'output_size': 3, ## Random\n",
        "                    'seq_len': 5, ## Random\n",
        "                    'hidden_state_size': 16,\n",
        "                    'num_layers': 1, \n",
        "                    'bidirectional': True, \n",
        "                    'dropout': 0.,\n",
        "                    'cell_type': 'LSTM', ## VanillaRNN, LSTM, GRU\n",
        "                    'device': 'cpu', ## 'cuda', 'cpu'\n",
        "                    'rnn_attn_type': 'Additive', ## Additive, Scaled-Dot, Linear, DA-RNN, None\n",
        "                    'num_epochs': 100, \n",
        "                    'lr':1e-3,\n",
        "                    'lr_decay':0.999,\n",
        "                    'batch_size': 16,\n",
        "                    'early_stopping_patience': 5,\n",
        "                    'print': True\n",
        "      }\n",
        "opts.update(args_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLUP4aBFg--z"
      },
      "source": [
        "k = 5\n",
        "t = 7\n",
        "out_features = ['Deaths']\n",
        "in_features = BASIC_FEATURES + TEST_FEATURES + HOSPITAL_FEATURES + LTC_FEATURES\n",
        "X, X_tilde, y, y_cumul, dates, in_features = cluster_data(df, k=5, t=7, separate=True, in_features=in_features, out_features=out_features, moving_average=True, m=3)\n",
        "X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, X_test, X_tilde_test, y_test = train_val_test_split(X, X_tilde, y, dates)\n",
        "\n",
        "opts.input_size = X_train.shape[2]\n",
        "opts.output_size = y_train.shape[1]\n",
        "opts.seq_len = X_train.shape[1]\n",
        "losses = train(X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KdQjzo6nbeg"
      },
      "source": [
        "model = torch.load('rnn_w_attn.pt')\n",
        "model(X_train.to(opts.device), X_tilde_train.to(opts.device))\n",
        "attn_plot_args = AttrDict()\n",
        "plot_num = 1\n",
        "plt_args = {\n",
        "          'grid_on': True,\n",
        "          'height': 5,\n",
        "          'width': 5,\n",
        "          'plot_num': plot_num,  # should be a value <= X_train.shape[0] or X_val.shape[0] and >= 1\n",
        "          'layer_num': 1, # should be a value <= num layers and >= 1\n",
        "          'in_features': in_features,\n",
        "          'plt_forward': True,\n",
        "          'plt_backward': True,\n",
        "          'model_type': 'rnn',\n",
        "          'k': k,\n",
        "          't': t,\n",
        "          'pred_date': dates[plot_num],\n",
        "          'plt_temporal': True\n",
        "} \n",
        "attn_plot_args.update(plt_args)\n",
        "visualize_attention(model, attn_plot_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vTo_G7EpM3rz"
      },
      "source": [
        "## Run Encoder-Decoder Architecture with Encoder and Decoder Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DGzYuOAssOSC"
      },
      "source": [
        "opts = AttrDict()\n",
        "args_dict = { 'name': 'ed_w_all_attn',\n",
        "              'model':'ED', ## R\n",
        "              'rnn_class': None,\n",
        "              'encoder_class': BidirectionalEncoderWithAttention, ## EncoderWithoutAttention, EncoderWithAttention (only for EncoderDecoder)\n",
        "              'decoder_class': DecoderWithAttention, ## DecoderWithAttention, DecoderWithoutAttention (only for EncoderDecoder)\n",
        "              'temporal_attn': True,\n",
        "              'bidirectional': True,\n",
        "              'input_size': 15, ## to be replaced\n",
        "              'output_size': 1, ## to be replaced\n",
        "              'seq_len': 5, ## to be replaced\n",
        "              'dropout': 0.,\n",
        "              'hidden_state_size': 32,\n",
        "              'encoder_num_hidden_layers': 1,\n",
        "              'decoder_num_hidden_layers': 1,\n",
        "              'cell_type': 'LSTM',\n",
        "              'enc_attn_type': 'Additive', ## Additive, Scaled-Dot, Linear, DA-RNN, None\n",
        "              'dec_attn_type': 'Additive',\n",
        "              'device': 'cpu', ## 'cuda'\n",
        "              'num_epochs': 150,\n",
        "              'lr':1e-3,\n",
        "              'batch_size': 32,\n",
        "              'lr_decay': 0.99, # or maybe 0.9\n",
        "              'early_stopping_patience': 10,\n",
        "              'print': True\n",
        "}\n",
        "opts.update(args_dict)\n",
        "\n",
        "out_features = ['Deaths']\n",
        "X, X_tilde, y, y_cumul, dates, in_features = cluster_data(df, k=5, t=7, separate=True, in_features=None, out_features=out_features, moving_average=True, m=3)\n",
        "X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, X_test, X_tilde_test, y_test = train_val_test_split(X, X_tilde, y, dates)\n",
        "\n",
        "opts.input_size = X_train.shape[2]\n",
        "opts.output_size = y_train.shape[1]\n",
        "opts.seq_len = X_train.shape[1]\n",
        "\n",
        "losses = train(X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, opts)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMmnzz8LW_RO"
      },
      "source": [
        "model = torch.load('ed_w_attn.pt')\n",
        "\n",
        "plt.plot(losses['epoch'], losses['val'])\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.show()\n",
        "\n",
        "args_dict = opts\n",
        "pred = model(X_train.to(args_dict.device), X_tilde_train.to(args_dict.device))\n",
        "pred = pred.cpu().detach().numpy()\n",
        "plt.figure()\n",
        "plt.plot(y_train, label=\"truth\")\n",
        "plt.plot(pred, label=\"pred\")\n",
        "plt.title(\"Cumulative Training Deaths\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "X_tilde_train_last_day_of_condition = X_tilde_train[:, -1, 0].cpu().detach().numpy()\n",
        "pred_diff = pred[:, 0] - X_tilde_train_last_day_of_condition\n",
        "y_train_diff = y_train[:, 0] - X_tilde_train_last_day_of_condition\n",
        "plt.figure()\n",
        "plt.plot(y_train_diff, label=\"truth\")\n",
        "plt.plot(pred_diff, label=\"pred\")\n",
        "plt.title(\"New Training Deaths\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "pred = model(X_val.to(args_dict.device), X_tilde_val.to(args_dict.device))\n",
        "pred = pred.cpu().detach().numpy()\n",
        "plt.figure()\n",
        "plt.plot(y_val, label=\"truth\")\n",
        "plt.plot(pred, label=\"pred\")\n",
        "plt.title(\"Cumulative Validation Deaths\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "X_tilde_val_last_day_of_condition = X_tilde_val[:, -1, 0].cpu().detach().numpy()\n",
        "pred_diff = pred[:, 0] - X_tilde_val_last_day_of_condition\n",
        "y_val_diff = y_val[:, 0] - X_tilde_val_last_day_of_condition\n",
        "plt.figure()\n",
        "plt.plot(y_val_diff, label=\"truth\")\n",
        "plt.plot(pred_diff, label=\"pred\")\n",
        "plt.title(\"New Training Deaths\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XRMjkTWpyduu"
      },
      "source": [
        "## Test Attention Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DKYYiDF0uwYX"
      },
      "source": [
        "model(X_train.to(opts.device), X_tilde_train.to(opts.device))\n",
        "attn_plot_args = AttrDict()\n",
        "plot_num = 2\n",
        "plt_args = {\n",
        "          'grid_on': True,\n",
        "          'height': 5,\n",
        "          'width': 5,\n",
        "          'plot_num': plot_num,  # should be a value <= X_train.shape[0] or X_val.shape[0] and >= 1\n",
        "          'layer_num': 1, # should be a value <= num layers and >= 1\n",
        "          'in_features': in_features,\n",
        "          'plt_encoder': True,\n",
        "          'plt_temporal': True,\n",
        "          'plt_forward': True,\n",
        "          'plt_backward': True,\n",
        "          'model_type': 'ed',\n",
        "          'k': 5,\n",
        "          't': 7,\n",
        "          'pred_date': dates[plot_num]\n",
        "} \n",
        "attn_plot_args.update(plt_args)\n",
        "visualize_attention(model, attn_plot_args)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxqu1UeZKXjC"
      },
      "source": [
        "# Optimal Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_aKBU_T0Krnq"
      },
      "source": [
        "## RNN Without Input or Temporal Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5dtrp9zKOZH"
      },
      "source": [
        "rnn_wo_attn_args = AttrDict()\n",
        "args_dict = {       'name': 'rnn_wo_attn',\n",
        "                    'model':'R', ## R, ED\n",
        "                    'rnn_class': RNNWithoutAttention,\n",
        "                    'rnn_attn_type': None,\n",
        "                    'temporal_attn': False,\n",
        "                    'input_size': 15, ## Random -- to be set in the loop\n",
        "                    'output_size': 3, ## Random\n",
        "                    'seq_len': 5, ## Random\n",
        "                    'hidden_state_size': 16,\n",
        "                    'num_layers': 1, \n",
        "                    'bidirectional': True, \n",
        "                    'dropout': 0.,\n",
        "                    'cell_type': 'LSTM', ## VanillaRNN, LSTM, GRU\n",
        "                    'device': 'cpu', ## 'cuda', 'cpu'\n",
        "                    'num_epochs': 70, \n",
        "                    'rnn_attn_type': None,\n",
        "                    'lr':1e-3,\n",
        "                    'lr_decay':0.999,\n",
        "                    'batch_size': 16,\n",
        "                    'early_stopping_patience': 5,\n",
        "                    'print': True\n",
        "      }\n",
        "rnn_wo_attn_args.update(args_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM7n2ul8KuA4"
      },
      "source": [
        "## RNN With Input Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZEOpwT09KaoN"
      },
      "source": [
        "rnn_w_input_attn_args = AttrDict()\n",
        "args_dict = {       'name': 'rnn_w_input_attn',\n",
        "                    'model':'R', ## R, ED\n",
        "                    'rnn_class': RNNWithAttention, ## RNNWithoutAttention\n",
        "                    'temporal_attn': False,\n",
        "                    'input_size': 15, ## Random -- to be set in the loop\n",
        "                    'output_size': 3, ## Random\n",
        "                    'seq_len': 5, ## Random\n",
        "                    'hidden_state_size': 16,\n",
        "                    'num_layers': 1, \n",
        "                    'bidirectional': True, \n",
        "                    'dropout': 0.,\n",
        "                    'cell_type': 'LSTM', ## VanillaRNN, LSTM, GRU\n",
        "                    'device': 'cpu', ## 'cuda', 'cpu'\n",
        "                    'rnn_attn_type': 'Additive', ## Additive, Scaled-Dot, Linear, DA-RNN, None\n",
        "                    'num_epochs': 100, \n",
        "                    'lr':1e-3,\n",
        "                    'lr_decay':0.999,\n",
        "                    'batch_size': 16,\n",
        "                    'early_stopping_patience': 5,\n",
        "                    'print': True\n",
        "      }\n",
        "rnn_w_input_attn_args.update(args_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JE1MnQUN9y5-"
      },
      "source": [
        "## RNN With Temporal Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C0dwro5191cg"
      },
      "source": [
        "rnn_w_temp_attn_args = AttrDict()\n",
        "args_dict = {       'name': 'rnn_w_temp_attn',\n",
        "                    'model':'R', ## R, ED\n",
        "                    'rnn_class': RNNWithoutAttention, ## RNNWithoutAttention\n",
        "                    'temporal_attn': True,\n",
        "                    'input_size': 15, ## Random -- to be set in the loop\n",
        "                    'output_size': 3, ## Random\n",
        "                    'seq_len': 5, ## Random\n",
        "                    'hidden_state_size': 16,\n",
        "                    'num_layers': 1, \n",
        "                    'bidirectional': True, \n",
        "                    'dropout': 0.,\n",
        "                    'cell_type': 'LSTM', ## VanillaRNN, LSTM, GRU\n",
        "                    'device': 'cpu', ## 'cuda', 'cpu'\n",
        "                    'rnn_attn_type': None, ## Additive, Scaled-Dot, Linear, DA-RNN, None\n",
        "                    'num_epochs': 100, \n",
        "                    'lr':1e-3,\n",
        "                    'lr_decay':0.999,\n",
        "                    'batch_size': 32,\n",
        "                    'early_stopping_patience': 5,\n",
        "                    'print': True\n",
        "      }\n",
        "rnn_w_temp_attn_args.update(args_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lXzmeLy498Sv"
      },
      "source": [
        "## RNN With Input and Temporal Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kNCMhrXO9-bQ"
      },
      "source": [
        "rnn_w_attn_args = AttrDict()\n",
        "args_dict = {       'name': 'rnn_w_attn',\n",
        "                    'model':'R', ## R, ED\n",
        "                    'rnn_class': RNNWithAttention, ## RNNWithoutAttention\n",
        "                    'temporal_attn': True,\n",
        "                    'input_size': 15, ## Random -- to be set in the loop\n",
        "                    'output_size': 3, ## Random\n",
        "                    'seq_len': 5, ## Random\n",
        "                    'hidden_state_size': 16,\n",
        "                    'num_layers': 1, \n",
        "                    'bidirectional': True, \n",
        "                    'dropout': 0.,\n",
        "                    'cell_type': 'LSTM', ## VanillaRNN, LSTM, GRU\n",
        "                    'device': 'cpu', ## 'cuda', 'cpu'\n",
        "                    'rnn_attn_type': 'Additive', ## Additive, Scaled-Dot, Linear, DA-RNN, None\n",
        "                    'num_epochs': 100, \n",
        "                    'lr':1e-3,\n",
        "                    'lr_decay':0.999,\n",
        "                    'batch_size': 16,\n",
        "                    'early_stopping_patience': 5,\n",
        "                    'print': True\n",
        "      }\n",
        "rnn_w_attn_args.update(args_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXPn6V8BKw4b"
      },
      "source": [
        "## EncoderDecoder with no Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1RUjC2CfK11c"
      },
      "source": [
        "# this one is a bit noisy. I'm trying to make it more consistent still\n",
        "ed_wo_attn_args = AttrDict()\n",
        "args_dict = { 'name': 'ed_wo_attn',\n",
        "              'model':'ED', ## R\n",
        "              'rnn_class': None,\n",
        "              'encoder_class': EncoderWithoutAttention, ## EncoderWithoutAttention, EncoderWithAttention (only for EncoderDecoder)\n",
        "              'decoder_class': DecoderWithoutAttention, ## DecoderWithAttention, DecoderWithoutAttention (only for EncoderDecoder)\n",
        "              'temporal_attn': False,\n",
        "              'bidirectional': True,\n",
        "              'input_size': 15, ## to be replaced\n",
        "              'output_size': 1, ## to be replaced\n",
        "              'seq_len': 5, ## to be replaced\n",
        "              'dropout': 0.,\n",
        "              'hidden_state_size': 20, #12,\n",
        "              'encoder_num_hidden_layers': 1,\n",
        "              'decoder_num_hidden_layers': 1,\n",
        "              'cell_type': 'LSTM',\n",
        "              'enc_attn_type': None, ## Additive, Scaled-Dot, Linear, DA-RNN, None\n",
        "              'dec_attn_type': None,\n",
        "              'device': 'cpu', ## 'cuda'\n",
        "              'num_epochs': 300,\n",
        "              'lr':1e-3,\n",
        "              'batch_size': 22, #32,\n",
        "              'lr_decay': 0.8,\n",
        "              'early_stopping_patience': 20,\n",
        "              'print': True\n",
        "}\n",
        "ed_wo_attn_args.update(args_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0c7YjpdLdx4"
      },
      "source": [
        "## EncoderDecoder with Encoder Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rb9eXEyeLhcm"
      },
      "source": [
        "# I ran this several times, and it seemed to produce good results. However, there is a noticeable amount of variance\n",
        "# Also, maybe its a bit noisy? --> might have to fine-tune a bit more. \n",
        "enc_w_attn_args = AttrDict()\n",
        "args_dict = { 'name': 'enc_w_attn',\n",
        "              'model':'ED', ## R\n",
        "              'rnn_class': None,\n",
        "              'encoder_class': BidirectionalEncoderWithAttention, ## EncoderWithoutAttention, EncoderWithAttention (only for EncoderDecoder)\n",
        "              'decoder_class': DecoderWithoutAttention, ## DecoderWithAttention, DecoderWithoutAttention (only for EncoderDecoder)\n",
        "              'temporal_attn': False,\n",
        "              'bidirectional': True,\n",
        "              'input_size': 15, ## to be replaced\n",
        "              'output_size': 1, ## to be replaced\n",
        "              'seq_len': 5, ## to be replaced\n",
        "              'dropout': 0.,\n",
        "              'hidden_state_size': 20,\n",
        "              'encoder_num_hidden_layers': 1,\n",
        "              'decoder_num_hidden_layers': 1,\n",
        "              'cell_type': 'LSTM',\n",
        "              'enc_attn_type': 'Additive', ## Additive, Scaled-Dot, Linear, DA-RNN, None\n",
        "              'dec_attn_type': None,\n",
        "              'device': 'cpu', ## 'cuda'\n",
        "              'num_epochs': 200,\n",
        "              'lr':1e-3,\n",
        "              'batch_size': 32,\n",
        "              'lr_decay': 0.97,\n",
        "              'early_stopping_patience': 20,\n",
        "              'print': True\n",
        "}\n",
        "enc_w_attn_args.update(args_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lRzFdCzlLuf6"
      },
      "source": [
        "## EncoderDecoder with Decoder Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBphjWmsLw91"
      },
      "source": [
        "# These hyper params seem not that noisy, but variance is noticeable. \n",
        "dec_w_attn_args = AttrDict()\n",
        "args_dict = { 'name': 'dec_w_attn',\n",
        "              'model':'ED', ## R\n",
        "              'rnn_class': None,\n",
        "              'encoder_class': EncoderWithoutAttention, ## EncoderWithoutAttention, EncoderWithAttention (only for EncoderDecoder)\n",
        "              'decoder_class': DecoderWithAttention, ## DecoderWithAttention, DecoderWithoutAttention (only for EncoderDecoder)\n",
        "              'temporal_attn': False,\n",
        "              'bidirectional': True,\n",
        "              'input_size': 15, ## to be replaced\n",
        "              'output_size': 1, ## to be replaced\n",
        "              'seq_len': 5, ## to be replaced\n",
        "              'dropout': 0.,\n",
        "              'hidden_state_size': 26,\n",
        "              'encoder_num_hidden_layers': 1,\n",
        "              'decoder_num_hidden_layers': 1,\n",
        "              'cell_type': 'LSTM',\n",
        "              'enc_attn_type': None, ## Additive, Scaled-Dot, Linear, DA-RNN, None\n",
        "              'dec_attn_type': 'Additive',\n",
        "              'device': 'cpu', ## 'cuda'\n",
        "              'num_epochs': 200,\n",
        "              'lr':1e-3,\n",
        "              'batch_size': 32,\n",
        "              'lr_decay': 0.80,\n",
        "              'early_stopping_patience': 20,\n",
        "              'print': True\n",
        "}\n",
        "dec_w_attn_args.update(args_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGkPGTkNL-Gy"
      },
      "source": [
        "## EncoderDecoder: Encoder with Attention & Decoder with Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GMg0tZ56MARY"
      },
      "source": [
        "ed_w_attn_args = AttrDict()\n",
        "args_dict = { 'name': 'ed_w_attn',\n",
        "              'model':'ED', ## R\n",
        "              'rnn_class': None,\n",
        "              'encoder_class': BidirectionalEncoderWithAttention, ## EncoderWithoutAttention, EncoderWithAttention (only for EncoderDecoder)\n",
        "              'decoder_class': DecoderWithAttention, ## DecoderWithAttention, DecoderWithoutAttention (only for EncoderDecoder)\n",
        "              'temporal_attn': False,\n",
        "              'bidirectional': True,\n",
        "              'input_size': 15, ## to be replaced\n",
        "              'output_size': 1, ## to be replaced\n",
        "              'seq_len': 5, ## to be replaced\n",
        "              'dropout': 0.,\n",
        "              'hidden_state_size': 16,\n",
        "              'encoder_num_hidden_layers': 1,\n",
        "              'decoder_num_hidden_layers': 1,\n",
        "              'cell_type': 'LSTM',\n",
        "              'enc_attn_type': 'Additive', ## Additive, Scaled-Dot, Linear, DA-RNN, None\n",
        "              'dec_attn_type': 'Scaled-Dot',\n",
        "              'device': 'cpu', ## 'cuda'\n",
        "              'num_epochs': 200,\n",
        "              'lr':1e-3,\n",
        "              'batch_size': 32,\n",
        "              'lr_decay': 0.95,\n",
        "              'early_stopping_patience': 10,\n",
        "              'print': True\n",
        "}\n",
        "ed_w_attn_args.update(args_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1jLNjyAr-eaX"
      },
      "source": [
        "## EncoderDecoder with Temporal Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qm3h_m0c-lCU"
      },
      "source": [
        "ed_w_temp_attn_args = AttrDict()\n",
        "args_dict = { 'name': 'ed_w_temp_attn',\n",
        "              'model':'ED', ## R\n",
        "              'rnn_class': None,\n",
        "              'encoder_class': EncoderWithoutAttention, ## EncoderWithoutAttention, EncoderWithAttention (only for EncoderDecoder)\n",
        "              'decoder_class': DecoderWithoutAttention, ## DecoderWithAttention, DecoderWithoutAttention (only for EncoderDecoder)\n",
        "              'temporal_attn': True,\n",
        "              'bidirectional': True,\n",
        "              'input_size': 15, ## to be replaced\n",
        "              'output_size': 1, ## to be replaced\n",
        "              'seq_len': 5, ## to be replaced\n",
        "              'dropout': 0.,\n",
        "              'hidden_state_size': 16,\n",
        "              'encoder_num_hidden_layers': 1,\n",
        "              'decoder_num_hidden_layers': 1,\n",
        "              'cell_type': 'LSTM',\n",
        "              'enc_attn_type': None, ## Additive, Scaled-Dot, Linear, DA-RNN, None\n",
        "              'dec_attn_type': None,\n",
        "              'device': 'cpu', ## 'cuda'\n",
        "              'num_epochs': 100,\n",
        "              'lr':1e-3,\n",
        "              'batch_size': 32,\n",
        "              'lr_decay': 0.85,\n",
        "              'early_stopping_patience': 20,\n",
        "              'print': True\n",
        "}\n",
        "ed_w_temp_attn_args.update(args_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCsgZPVD-vD8"
      },
      "source": [
        "## EncoderDecoder with Encoder and Temporal Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qCiOONVg-yf0"
      },
      "source": [
        "enc_and_temp_w_attn_args = AttrDict()\n",
        "args_dict = { 'name': 'enc_and_temp_w_attn',\n",
        "              'model':'ED', ## R\n",
        "              'rnn_class': None,\n",
        "              'encoder_class': BidirectionalEncoderWithAttention, ## EncoderWithoutAttention, EncoderWithAttention (only for EncoderDecoder)\n",
        "              'decoder_class': DecoderWithoutAttention, ## DecoderWithAttention, DecoderWithoutAttention (only for EncoderDecoder)\n",
        "              'temporal_attn': True,\n",
        "              'bidirectional': True,\n",
        "              'input_size': 15, ## to be replaced\n",
        "              'output_size': 1, ## to be replaced\n",
        "              'seq_len': 5, ## to be replaced\n",
        "              'dropout': 0.,\n",
        "              'hidden_state_size': 48,\n",
        "              'encoder_num_hidden_layers': 1,\n",
        "              'decoder_num_hidden_layers': 1,\n",
        "              'cell_type': 'LSTM',\n",
        "              'enc_attn_type': 'Additive', ## Additive, Scaled-Dot, Linear, DA-RNN, None --- Not Additive or Scaled-Dot works best\n",
        "              'dec_attn_type': None,\n",
        "              'device': 'cpu', ## 'cuda'\n",
        "              'num_epochs': 100,\n",
        "              'lr':1e-3,\n",
        "              'batch_size': 10,\n",
        "              'lr_decay': 0.925,\n",
        "              'early_stopping_patience': 20,\n",
        "              'print': True\n",
        "}\n",
        "enc_and_temp_w_attn_args.update(args_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0-6mMk9-7oP"
      },
      "source": [
        "## EncoderDecoder with Decoder and Temporal Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBLyMWXn--sL"
      },
      "source": [
        "dec_and_temp_w_attn_args = AttrDict()\n",
        "args_dict = { 'name': 'dec_and_temp_w_attn',\n",
        "              'model':'ED', ## R\n",
        "              'rnn_class': None,\n",
        "              'encoder_class': EncoderWithoutAttention, ## EncoderWithoutAttention, EncoderWithAttention (only for EncoderDecoder)\n",
        "              'decoder_class': DecoderWithAttention, ## DecoderWithAttention, DecoderWithoutAttention (only for EncoderDecoder)\n",
        "              'temporal_attn': True,\n",
        "              'bidirectional': True,\n",
        "              'input_size': 15, ## to be replaced\n",
        "              'output_size': 1, ## to be replaced\n",
        "              'seq_len': 5, ## to be replaced\n",
        "              'dropout': 0.,\n",
        "              'hidden_state_size': 18,\n",
        "              'encoder_num_hidden_layers': 1,\n",
        "              'decoder_num_hidden_layers': 1,\n",
        "              'cell_type': 'LSTM',\n",
        "              'enc_attn_type': None, ## Additive, Scaled-Dot, Linear, DA-RNN, None\n",
        "              'dec_attn_type': 'Additive', \n",
        "              'device': 'cpu', ## 'cuda'\n",
        "              'num_epochs': 100,\n",
        "              'lr':1e-3,\n",
        "              'batch_size': 12,\n",
        "              'lr_decay': 0.70,\n",
        "              'early_stopping_patience': 20, \n",
        "              'print': True\n",
        "}\n",
        "dec_and_temp_w_attn_args.update(args_dict)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q4I0H1ax_kCq"
      },
      "source": [
        "## EncoderDecoder with All Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EA7nMTjE_mat"
      },
      "source": [
        "ed_w_all_attn_args = AttrDict()\n",
        "args_dict = { 'name': 'ed_w_all_attn',\n",
        "              'model':'ED', ## R\n",
        "              'rnn_class': None,\n",
        "              'encoder_class': BidirectionalEncoderWithAttention, ## EncoderWithoutAttention, EncoderWithAttention (only for EncoderDecoder)\n",
        "              'decoder_class': DecoderWithAttention, ## DecoderWithAttention, DecoderWithoutAttention (only for EncoderDecoder)\n",
        "              'temporal_attn': True,\n",
        "              'bidirectional': True,\n",
        "              'input_size': 15, ## to be replaced\n",
        "              'output_size': 1, ## to be replaced\n",
        "              'seq_len': 5, ## to be replaced\n",
        "              'dropout': 0.,\n",
        "              'hidden_state_size': 32,\n",
        "              'encoder_num_hidden_layers': 1,\n",
        "              'decoder_num_hidden_layers': 1,\n",
        "              'cell_type': 'LSTM',\n",
        "              'enc_attn_type': 'Additive', ## Additive, Scaled-Dot, Linear, DA-RNN, None\n",
        "              'dec_attn_type': 'Additive',\n",
        "              'device': 'cpu', ## 'cuda'\n",
        "              'num_epochs': 150,\n",
        "              'lr':1e-3,\n",
        "              'batch_size': 32,\n",
        "              'lr_decay': 0.99, # or maybe 0.9\n",
        "              'early_stopping_patience': 10,\n",
        "              'print': True\n",
        "}\n",
        "ed_w_all_attn_args.update(args_dict)\n",
        "# most of the runs seem fine with these parameters, but consistency is sometimes a problem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19hgkIBWd47J"
      },
      "source": [
        "# Experiments 1 and 2: Model Performance"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e4ADm_TtInW8"
      },
      "source": [
        "\"\"\"\n",
        "BASIC_FEATURES = ['Positive', 'Total Cases', 'Resolved', 'Deaths']\n",
        "TEST_FEATURES = ['AFT','TTC', 'PPT', 'UI']\n",
        "HOSPITAL_FEATURES = ['Hospital', 'ICU', 'Ventilator']\n",
        "LTC_FEATURES = ['TPLTCR', 'TPLTCHCW', 'TLCTRD', 'TLTCHCWD']\n",
        "\"\"\"\n",
        "\n",
        "OUT_FEATURES = ['Deaths']\n",
        "ALL_IN_FEATURES = BASIC_FEATURES + TEST_FEATURES + HOSPITAL_FEATURES + LTC_FEATURES\n",
        "ALL_FEATURES = [BASIC_FEATURES, TEST_FEATURES, HOSPITAL_FEATURES, LTC_FEATURES]\n",
        "ALL_FEATURE_NAMES = ['Basic', 'Testing', 'Hospital', 'LTC']\n",
        "MODEL_NAMES = ['rnn_wo_attn', 'rnn_w_input_attn', 'rnn_w_temp_attn', 'rnn_w_attn', \n",
        "               'ed_wo_attn', 'dec_w_attn', 'enc_w_attn', 'ed_w_attn', 'ed_w_temp_attn',\n",
        "               'enc_and_temp_w_attn', 'dec_and_temp_w_attn', 'ed_w_all_attn']\n",
        "MODEL_ARGS_LST = [rnn_wo_attn_args, rnn_w_input_attn_args, rnn_w_temp_attn_args, \n",
        "                  rnn_w_attn_args, ed_wo_attn_args, dec_w_attn_args, enc_w_attn_args, \n",
        "                  ed_w_attn_args, ed_w_temp_attn_args, enc_and_temp_w_attn_args, \n",
        "                  dec_and_temp_w_attn_args, ed_w_all_attn_args]\n",
        "\n",
        "\n",
        "def experiments_1_and_2():\n",
        "    \"\"\"\n",
        "    Model performance\n",
        "    model_args_lst: The AttrDict opts for the models in order\n",
        "    \"\"\"\n",
        "    feature_names_lst = ALL_FEATURE_NAMES\n",
        "    feature_lst = ALL_FEATURES\n",
        "    in_features = ALL_IN_FEATURES\n",
        "    out_features = OUT_FEATURES\n",
        "\n",
        "    loss_dict = {'model': [], 'mse means': [], 'mse stds': [], 'mae means': [], 'mae stds': []}\n",
        "\n",
        "    mae_loss_fn = nn.L1Loss()\n",
        "    for i, model_name in enumerate(MODEL_NAMES):\n",
        "        args_dict = MODEL_ARGS_LST[i]\n",
        "        args_dict.print = False\n",
        "\n",
        "        ## Remove output features from encoder input features if encoder-decoder\n",
        "        if model_name[:3] != \"rnn\" and set(in_features) & set(out_features):\n",
        "            in_features_cpy = in_features.copy()\n",
        "            for f in out_features:\n",
        "                if f in in_features_cpy:\n",
        "                    in_features_cpy.remove(f)\n",
        "        else:\n",
        "            in_features_cpy = in_features\n",
        "\n",
        "        X, X_tilde, y, y_cumul, dates, _ = cluster_data(df, k=5, t=7, separate=True, in_features=in_features_cpy, out_features=out_features, moving_average=True, m=3)\n",
        "        X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, X_test, X_tilde_test, y_test = train_val_test_split(X, X_tilde, y, dates)\n",
        "\n",
        "        args_dict.input_size = X_train.shape[2]\n",
        "        args_dict.output_size = y_train.shape[1]\n",
        "        args_dict.seq_len = X_train.shape[1]\n",
        "        \n",
        "        min_test_losses = np.zeros(10)\n",
        "        min_test_maes = np.zeros(10)\n",
        "        for j in range(10):\n",
        "            train(X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, args_dict)\n",
        "            model = torch.load('{}.pt'.format(model_name))\n",
        "            y_hat = model(X_test.to(args_dict.device), X_tilde_test.to(args_dict.device))\n",
        "            min_test_losses[j] = compute_loss(y_hat, y_test.to(args_dict.device), nn.MSELoss())\n",
        "            min_test_maes[j] = mae_loss_fn(y_hat, y_test.to(args_dict.device))\n",
        "        mse_mean, mse_std = np.mean(min_test_losses), np.std(min_test_losses) \n",
        "        mae_mean, mae_std = np.mean(min_test_maes), np.std(min_test_maes)\n",
        "        loss_dict['mse means'].append(mse_mean)\n",
        "        loss_dict['mse stds'].append(mse_std)\n",
        "        loss_dict['mae means'].append(mae_mean)\n",
        "        loss_dict['mae stds'].append(mae_std)\n",
        "        loss_dict['model'].append(model_name)\n",
        "        print(\"Model: {} | Mean MSE: {} | MSE Std: {} | Mean MAE: {} | MAE Std: {}\".format(model_name, mse_mean, mse_std, mae_mean, mae_std))\n",
        "\n",
        "    return loss_dict\n",
        "\n",
        "def experiments_1_and_2_plot(loss_dict):\n",
        "    \"\"\"Plot a bar graph for model performances \"\"\"\n",
        "\n",
        "    fig, ax = plt.subplots()\n",
        "    y_pos = np.arange(len(MODEL_NAMES))\n",
        "    ax.barh(y_pos, loss_dict['mse means'], xerr = loss_dict['mse stds'], align = 'center')\n",
        "    ax.set_yticks(y_pos)\n",
        "    ax.set_yticklabels(MODEL_NAMES)\n",
        "    ax.invert_yaxis()\n",
        "    \n",
        "    ax.set_xlabel('Mean Test Loss Over 10 Runs')\n",
        "    plt.savefig('experiment1/plot.pdf')\n",
        "\n",
        "\n",
        "def print_loss_summary(loss_dict):\n",
        "    print('Model \\t Test MSE (std) \\t Test MAE (std)')\n",
        "    for i, model_name in enumerate(loss_dict['model']):\n",
        "        print('{} | {} ({}) | {} ({})'.format(model_name, loss_dict['mse means'][i], \n",
        "                                              loss_dict['mse stds'][i], loss_dict['mae means'][i], \n",
        "                                              loss_dict['mae stds'][i]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh5jNaphla_i"
      },
      "source": [
        "loss_dict = experiments_1_and_2()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tx4aJvrFlhoE"
      },
      "source": [
        "experiments_1_and_2_plot(loss_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3wkDS25JI7s"
      },
      "source": [
        "print_loss_summary(loss_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Hi2I3qT6pNB"
      },
      "source": [
        "# Experiment 3 Prelim: Input Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TJKryqws6ryC"
      },
      "source": [
        "OUT_FEATURES = ['Deaths']\n",
        "ALL_IN_FEATURES = BASIC_FEATURES + TEST_FEATURES + HOSPITAL_FEATURES + LTC_FEATURES\n",
        "ALL_FEATURES = [BASIC_FEATURES, TEST_FEATURES, HOSPITAL_FEATURES, LTC_FEATURES]\n",
        "ALL_FEATURE_NAMES = ['Basic', 'Testing', 'Hospital', 'LTC']\n",
        "MODEL_NAMES = ['rnn_wo_attn', 'rnn_w_temp_attn', 'ed_wo_attn', 'dec_w_attn', 'ed_w_temp_attn', 'dec_and_temp_w_attn']\n",
        "MODEL_ARGS_LST = [rnn_wo_attn_args, rnn_w_temp_attn_args, ed_wo_attn_args, dec_w_attn_args, ed_w_temp_attn_args, \n",
        "                  dec_and_temp_w_attn_args]\n",
        "\n",
        "\n",
        "def experiment3_prelim():\n",
        "    \"\"\"\n",
        "    Vary the input\n",
        "    model_args_lst: The AttrDict opts for the models in order\n",
        "    \"\"\"\n",
        "    feature_names_lst = ALL_FEATURE_NAMES\n",
        "    feature_lst = ALL_FEATURES\n",
        "    out_features = OUT_FEATURES\n",
        "\n",
        "    ## Get all feature combinations\n",
        "    feat_combo_lst = [list(name) for l in range(1, len(feature_names_lst) + 1) for name in combinations(feature_names_lst, l)]\n",
        "\n",
        "    feat_dict = dict()\n",
        "    for name in MODEL_NAMES:\n",
        "        feat_dict[name] = {'mean': [], 'std': []}\n",
        "\n",
        "    for i, model_name in enumerate(MODEL_NAMES):\n",
        "        args_dict = MODEL_ARGS_LST[i]\n",
        "        args_dict.print = False\n",
        "        args_dict.device = 'cuda'\n",
        "        for l in range(1, len(feature_lst) + 1):\n",
        "            for feats in combinations(feature_lst, l):\n",
        "                in_features = []\n",
        "                for f in feats:\n",
        "                    in_features.extend(f)\n",
        "\n",
        "                ## Remove output features from encoder input features if encoder-decoder\n",
        "                if model_name[:3] != \"rnn\" and set(in_features) & set(out_features):\n",
        "                    in_features_cpy = in_features.copy()\n",
        "                    for f in out_features:\n",
        "                        if f in in_features_cpy:\n",
        "                            in_features_cpy.remove(f)\n",
        "                else:\n",
        "                    in_features_cpy = in_features\n",
        "\n",
        "                X, X_tilde, y, y_cumul, dates, _ = cluster_data(df, k=5, t=7, separate=True, in_features=in_features_cpy, out_features=out_features, moving_average=True, m=3)\n",
        "                X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, X_test, X_tilde_test, y_test = train_val_test_split(X, X_tilde, y, dates)\n",
        "\n",
        "                args_dict.input_size = X_train.shape[2]\n",
        "                args_dict.output_size = y_train.shape[1]\n",
        "                args_dict.seq_len = X_train.shape[1]\n",
        "                \n",
        "                min_test_losses = np.zeros(5)\n",
        "                for j in range(5):\n",
        "                    train(X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, args_dict)\n",
        "                    model = torch.load('{}.pt'.format(model_name))\n",
        "                    y_hat = model(X_test.to(args_dict.device), X_tilde_test.to(args_dict.device))\n",
        "                    min_test_losses[j] = compute_loss(y_hat, y_test.to(args_dict.device), nn.MSELoss())\n",
        "                loss_mean = np.mean(min_test_losses)\n",
        "                loss_std = np.std(min_test_losses)\n",
        "                feat_dict[model_name]['mean'].append(loss_mean)\n",
        "                feat_dict[model_name]['std'].append(loss_std)\n",
        "                print(\"Model: {} | Mean: {} | Std: {}\".format(model_name, loss_mean, loss_std))\n",
        "\n",
        "    return feat_combo_lst, feat_dict\n",
        "\n",
        "def experiment3_prelim_plot(feat_combo_lst, feat_dict):\n",
        "    \"\"\"Plot a bar graph for each model of different combinations of features\"\"\"\n",
        "\n",
        "    for model in MODEL_NAMES:\n",
        "        fig, ax = plt.subplots()\n",
        "        y_pos = np.arange(len(feat_combo_lst))\n",
        "        ax.barh(y_pos, feat_dict[model]['mean'], xerr = feat_dict[model]['std'], align = 'center')\n",
        "        ax.set_yticks(y_pos)\n",
        "        ax.set_yticklabels(feat_combo_lst)\n",
        "        ax.invert_yaxis()\n",
        "        \n",
        "        ax.set_xlabel('Mean Test Loss Over 5 Runs')\n",
        "        ax.set_title('Experiment 3 Prelim: {}'.format(model))\n",
        "        plt.savefig('experiment3/{}.pdf'.format(model))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZP9qlrcJ7B0"
      },
      "source": [
        "feat_combo_lst, feat_dict = experiment3_prelim()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VEa88YyuJ-Zu"
      },
      "source": [
        "experiment3_prelim_plot(feat_combo_lst, feat_dict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmuIbs2U_fOO"
      },
      "source": [
        "# Experiment 3: Input Attention Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TPL3JB4v_uLR"
      },
      "source": [
        "## Experiment 3\n",
        "EXP3_MODEL_NAMES = ['rnn_w_input_attn', 'rnn_w_attn', 'enc_w_attn', 'ed_w_attn', 'enc_and_temp_w_attn', 'ed_w_all_attn']\n",
        "EXP3_MODEL_ARGS_LST = [rnn_w_input_attn_args, rnn_w_attn_args, enc_w_attn_args, ed_w_attn_args, enc_and_temp_w_attn_args, \n",
        "                  ed_w_all_attn_args]\n",
        "\n",
        "## Experiment 4\n",
        "EXP4_MODEL_NAMES = ['rnn_w_temp_attn', 'rnn_w_attn', 'ed_w_temp_attn',\n",
        "               'enc_and_temp_w_attn', 'dec_and_temp_w_attn', 'ed_w_all_attn']\n",
        "EXP4_MODEL_ARGS_LST = [rnn_w_temp_attn_args, rnn_w_attn_args, ed_w_temp_attn_args, enc_and_temp_w_attn_args, \n",
        "                  dec_and_temp_w_attn_args, ed_w_all_attn_args]\n",
        "\n",
        "def experiment_3_or_4_train(experiment3=True):\n",
        "\n",
        "    if experiment3:\n",
        "        MODEL_NAMES = EXP3_MODEL_NAMES\n",
        "        MODEL_ARGS_LST = EXP3_MODEL_ARGS_LST\n",
        "    else:\n",
        "        MODEL_NAMES = EXP4_MODEL_NAMES\n",
        "        MODEL_ARGS_LST = EXP4_MODEL_ARGS_LST\n",
        "\n",
        "    in_features = ALL_IN_FEATURES\n",
        "    out_features = OUT_FEATURES\n",
        "\n",
        "    X_lst, X_tilde_lst = [], []\n",
        "\n",
        "    for i, model_name in enumerate(MODEL_NAMES):\n",
        "        args_dict = MODEL_ARGS_LST[i]\n",
        "        args_dict.print = False\n",
        "        args_dict.device = 'cuda'\n",
        "\n",
        "        ## Remove output features from encoder input features if encoder-decoder\n",
        "        if model_name[:3] != \"rnn\" and set(in_features) & set(out_features):\n",
        "            in_features_cpy = in_features.copy()\n",
        "            for f in out_features:\n",
        "                if f in in_features_cpy:\n",
        "                    in_features_cpy.remove(f)\n",
        "        else:\n",
        "            in_features_cpy = in_features\n",
        "\n",
        "        X, X_tilde, y, y_cumul, dates, _ = cluster_data(df, k=5, t=7, separate=True, in_features=in_features_cpy, out_features=out_features, moving_average=True, m=3)\n",
        "        X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, X_test, X_tilde_test, y_test = train_val_test_split(X, X_tilde, y, dates)\n",
        "\n",
        "        args_dict.input_size = X_train.shape[2]\n",
        "        args_dict.output_size = y_train.shape[1]\n",
        "        args_dict.seq_len = X_train.shape[1]\n",
        "        \n",
        "        train(X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, args_dict)\n",
        "\n",
        "        if i == 0 or i == len(MODEL_NAMES) - 1: ## Save the inputs for rnn's and ed's\n",
        "            X_lst.append(X)\n",
        "            X_tilde_lst.append(X_tilde)\n",
        "        \n",
        "\n",
        "    return X_lst, X_tilde_lst, dates\n",
        "\n",
        "attn_plot_args = AttrDict()\n",
        "plt_args = {\n",
        "          'grid_on': True,\n",
        "          'height': 10,\n",
        "          'width': 10,\n",
        "          'plot_num': 1,  # should be a value <= dates.shape[0] and >= 0\n",
        "          'layer_num': 1, # should be a value <= num layers and >= 0\n",
        "          'in_features': ALL_IN_FEATURES,\n",
        "          'plt_encoder': True,\n",
        "          'plt_temporal': False,\n",
        "          'plt_forward': True,\n",
        "          'plt_backward': True,\n",
        "          'model_type': 'ed',\n",
        "          'k': 5,\n",
        "          't': 7,\n",
        "          'pred_date': None,\n",
        "          'model_name': None\n",
        "} \n",
        "attn_plot_args.update(plt_args)\n",
        "\n",
        "def experiment_3_or_4_attn_plots(X_lst, X_tilde_lst, dates, experiment3=True):\n",
        "    plot_nums = np.array([5, dates.shape[0] // 2, dates.shape[0] - 10])\n",
        "    if experiment3:\n",
        "        attn_plot_args.plt_encoder = True\n",
        "        attn_plot_args.plt_forward = True\n",
        "        attn_plot_args.plt_backward = True\n",
        "        attn_plot_args.plt_temporal = False\n",
        "\n",
        "        MODEL_NAMES = EXP3_MODEL_NAMES\n",
        "        MODEL_ARGS_LST = EXP3_MODEL_ARGS_LST\n",
        "    else:\n",
        "        attn_plot_args.plt_encoder = False\n",
        "        attn_plot_args.plt_forward = False\n",
        "        attn_plot_args.plt_backward = False\n",
        "        attn_plot_args.plt_temporal = True\n",
        "\n",
        "        MODEL_NAMES = EXP4_MODEL_NAMES\n",
        "        MODEL_ARGS_LST = EXP4_MODEL_ARGS_LST\n",
        "\n",
        "    for i, model_name in enumerate(MODEL_NAMES):\n",
        "        model = torch.load('{}.pt'.format(model_name))\n",
        "\n",
        "        attn_plot_args.model_name = model_name\n",
        "        if model_name[:3] == 'rnn':\n",
        "            j = 0\n",
        "            model_type = 'rnn'\n",
        "            attn_plot_args.in_features = ALL_IN_FEATURES\n",
        "        else:\n",
        "            j = 1\n",
        "            model_type = 'ed'\n",
        "            in_features_cpy = ALL_IN_FEATURES.copy()\n",
        "            in_features_cpy.remove('Deaths')\n",
        "            attn_plot_args.in_features = in_features_cpy\n",
        "\n",
        "        model(X_lst[j].float().to(MODEL_ARGS_LST[i].device), X_tilde_lst[j].float().to(MODEL_ARGS_LST[i].device))\n",
        "        \n",
        "        attn_plot_args.plot_num = plot_nums\n",
        "        attn_plot_args.model_type = model_type\n",
        "        attn_plot_args.pred_date = dates[plot_nums]\n",
        "\n",
        "        print(model_name)\n",
        "        visualize_attention(model, attn_plot_args)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3y8qzGKXL2_"
      },
      "source": [
        "X_lst, X_tilde_lst, dates = experiment_3_or_4_train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOpD3Z7HXQWe"
      },
      "source": [
        "experiment_3_or_4_attn_plots(X_lst, X_tilde_lst, dates)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E9gXml2_oBb"
      },
      "source": [
        "# Experiment 4: Temporal Attention Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5yOEUqTamF5"
      },
      "source": [
        "X_lst, X_tilde_lst, dates = experiment_3_or_4_train(experiment3=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4TXMkZft_vLy"
      },
      "source": [
        "experiment_3_or_4_attn_plots(X_lst, X_tilde_lst, dates, experiment3=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jZoCWQy4RgeH"
      },
      "source": [
        "# Truth vs. Predicted for Best Models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t94hf5GNR8sG"
      },
      "source": [
        "OUT_FEATURES = ['Deaths']\n",
        "ALL_IN_FEATURES = BASIC_FEATURES + TEST_FEATURES + HOSPITAL_FEATURES + LTC_FEATURES\n",
        "MODEL_NAMES = ['rnn_wo_attn', 'rnn_w_input_attn', 'rnn_w_temp_attn', 'rnn_w_attn', \n",
        "               'ed_wo_attn', 'dec_w_attn', 'enc_w_attn', 'ed_w_attn', 'ed_w_temp_attn',\n",
        "               'enc_and_temp_w_attn', 'dec_and_temp_w_attn', 'ed_w_all_attn']\n",
        "MODEL_ARGS_LST = [rnn_wo_attn_args, rnn_w_input_attn_args, rnn_w_temp_attn_args, \n",
        "                  rnn_w_attn_args, ed_wo_attn_args, dec_w_attn_args, enc_w_attn_args, \n",
        "                  ed_w_attn_args, ed_w_temp_attn_args, enc_and_temp_w_attn_args, \n",
        "                  dec_and_temp_w_attn_args, ed_w_all_attn_args]\n",
        "\n",
        "def visualize_predictions(model_name):\n",
        "    ## Train model\n",
        "    idx = MODEL_NAMES.index(model_name)\n",
        "    args_dict = MODEL_ARGS_LST[idx]\n",
        "    assert(args_dict.name == model_name)\n",
        "    args_dict.print = False\n",
        "    in_features = ALL_IN_FEATURES\n",
        "    out_features = OUT_FEATURES\n",
        "\n",
        "    ## Remove output features from encoder input features if encoder-decoder\n",
        "    if model_name[:3] != \"rnn\" and set(in_features) & set(out_features):\n",
        "        in_features_cpy = in_features.copy()\n",
        "        for f in out_features:\n",
        "            if f in in_features_cpy:\n",
        "                in_features_cpy.remove(f)\n",
        "    else:\n",
        "        in_features_cpy = in_features\n",
        "\n",
        "    X, X_tilde, y, y_cumul, dates, _ = cluster_data(df, k=5, t=7, separate=True, in_features=in_features_cpy, out_features=out_features, moving_average=True, m=3)\n",
        "    X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, X_test, X_tilde_test, y_test = train_val_test_split(X, X_tilde, y, dates)\n",
        "\n",
        "    args_dict.input_size = X_train.shape[2]\n",
        "    args_dict.output_size = y_train.shape[1]\n",
        "    args_dict.seq_len = X_train.shape[1]\n",
        "    \n",
        "    train(X_train, X_tilde_train, y_train, X_val, X_tilde_val, y_val, args_dict)\n",
        "    model = torch.load('{}.pt'.format(model_name))\n",
        "    y_hat_train = model(X_train.to(args_dict.device), X_tilde_train.to(args_dict.device))\n",
        "    y_hat_val = model(X_val.to(args_dict.device), X_tilde_val.to(args_dict.device))\n",
        "    y_hat_test = model(X_test.to(args_dict.device), X_tilde_test.to(args_dict.device))\n",
        "\n",
        "    reported_dates = df['Reported Date'].values\n",
        "    deaths = df['Deaths'].values\n",
        "    first_date = dates[0]\n",
        "    first_date_idx = np.where(reported_dates == first_date)[0][0]\n",
        "    jan_1_idx = np.where(reported_dates == '2021-01-01')[0][0]\n",
        "    march_1_idx = np.where(reported_dates == '2021-03-01')[0][0]\n",
        "    april_17_idx = np.where(reported_dates == '2021-04-17')[0][0]\n",
        "\n",
        "    ## Plot training\n",
        "    plt.figure()\n",
        "    y_train = deaths[first_date_idx : jan_1_idx]\n",
        "    plt.plot(y_train, label=\"truth\")\n",
        "    plt.plot(y_hat_train.cpu().detach().numpy(), label=\"predicted\")\n",
        "    plt.title(\"True and predicted cumulative deaths from May 19 to Dec. 31, 2020\")\n",
        "    plt.xlabel(\"Number of days since May 19, 2020\")\n",
        "    plt.ylabel(\"Cumulative Deaths\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    ## Plot validation\n",
        "    plt.figure()\n",
        "    y_val = deaths[jan_1_idx : march_1_idx]\n",
        "    plt.plot(y_val, label=\"truth\")\n",
        "    plt.plot(y_hat_val.cpu().detach().numpy(), label=\"predicted\")\n",
        "    plt.title(\"True and predicted cumulative deaths from Jan. 1 to Feb. 28, 2021\")\n",
        "    plt.xlabel(\"Number of days since Jan. 1, 2021\")\n",
        "    plt.ylabel(\"Cumulative Deaths\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n",
        "    ## Plot test\n",
        "    plt.figure()\n",
        "    y_test = deaths[march_1_idx : april_17_idx]\n",
        "    plt.plot(y_test, label=\"truth\")\n",
        "    plt.plot(y_hat_test.cpu().detach().numpy(), label=\"predicted\")\n",
        "    plt.title(\"True and predicted cumulative deaths from March 1 to April 16, 2021\")\n",
        "    plt.xlabel(\"Number of days since March 1, 2021\")\n",
        "    plt.ylabel(\"Cumulative Deaths\")\n",
        "    plt.legend()\n",
        "    plt.show()\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a3Z2LE-RxV6"
      },
      "source": [
        "## Encoder-Decoder With Temporal Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hkN0_2i5a3Q8"
      },
      "source": [
        "visualize_predictions('ed_w_temp_attn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rA4suD8uRlHz"
      },
      "source": [
        "## RNN Without Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9aL4p7O_a-lQ"
      },
      "source": [
        "visualize_predictions('rnn_wo_attn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nkrr4UULRu9n"
      },
      "source": [
        "## Encoder-Decoder With Decoder Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5d_Qan4bBdW"
      },
      "source": [
        "visualize_predictions('dec_w_attn')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Erip7FiURq-w"
      },
      "source": [
        "## Encoder-Decoder Without Attention"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrLZfqnfbDv0"
      },
      "source": [
        "visualize_predictions('ed_wo_attn')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}